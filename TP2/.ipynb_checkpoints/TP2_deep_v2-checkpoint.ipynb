{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning - Introduction à Pytorch\n",
    "\n",
    "\n",
    "## TP2 : Fonctions Dérivables\n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y9YOOHHhJKY",
    "outputId": "47e1d462-0547-428d-9fe4-32c9a3d7481d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  1.7.1\n",
      "Le calcul GPU est disponible ?  False\n",
      "sklearn  0.24.2\n",
      "numpy  1.19.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "print(\"sklearn \",sklearn.__version__)\n",
    "print(\"numpy \",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEYsBM9XRexh"
   },
   "source": [
    "Au TP précédent, nous avons vu comment implémenter une regression linéaire en utilisant les structures Tensor de PyTorch. Cependant, nous exploitions pas du tout la puissance de PyTorch qui permet de faciliter le calcul des gradients via de l'auto-dérivation. Dans le TP précédent nous avions défini un algorithme spécifique à de la regression pour un modèle (linéaire) et un coût (moindres carrés) figés, en définissant à la main le gradient du coût global pour l'ensemble des paramètres. Ce mode de programmation est très peu modulaire et est très difficilement étendable à des architectures plus complexes. Sachant que l'objectif est de développer des architectures neuronales avec des nombreux modules neuronaux enchaînés, il n'est pas possible de travailler de cette façon.\n",
    "\n",
    "Dans ce TP, nous allons voir comment décomposer les choses pour rendre le code plus facilement généralisable. L'objectif est de comprendre le fonctionnement interne de PyTorch (sans en utiliser encore les facilités offertes par l'utilisation d'un graphe de calcul), basé sur l'implémentation d'objets Function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eioLeZ5YRexi"
   },
   "source": [
    "## Fonctions\n",
    "\n",
    "\n",
    "$\\href{https://pytorch.org/docs/stable/}{\\texttt{PyTorch}}$ utilise une classe abstraite $\\href{https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function}{\\texttt{Function}}$ dont sont héritées toutes les fonctions et qui nécessite l'implémentation de ces deux méthodes :\n",
    "\n",
    "- méthode $\\texttt{forward(ctx, *inputs)}$ : calcule le résultat de l'application de la fonction\n",
    "- méthode $\\texttt{backward(ctx, *grad-outputs)}$ : calcule le gradient partiel par rapport à chaque entrée de la méthode $\\texttt{forward}$; le nombre de $\\texttt{grad-outputs}$ doit être égale aux nombre de sorties de $\\texttt{forward}$ (pourquoi ?) et le nombre de  \n",
    "sorties doit être égale aux nombres de $\\texttt{inputs}$ de $\\texttt{forward}$.\n",
    "\n",
    "\n",
    "Pour des raisons d'implémentation, les deux méthodes doivent être statiques. Le premier paramètre $\\texttt{ctx}$ permet de sauvegarder un contexte lors de la passe $\\texttt{forward}$ (par exemple les tenseurs d'entrées) et il est passé lors de la passe $\\texttt{backward}$ en paramètre afin de récupérer les valeurs. $\\textbf{Attention : }$ le contexte doit être unique pour chaque appel de $\\texttt{forward}$.\n",
    "\n",
    "Compléter le code ci-dessous pour créer des modules MSE (coût moindres carrés) et Linéaire. Les deux cellules en dessous vous serviront à tester votre code: si tout se passe sans plantage, alors vos gradients semblent corrects. Utiliser bien les outils propres à pyTorch, en particulier des Tensor et pas des matrices numpy. Assurez vous que\n",
    "vos fonctions prennent en entrée des batchs d’exemples (matrice 2D) et non un seul exemple (vecteur). N’hésiter pas à prendre un exemple et déterminer les dimensions des différentes matrices en jeu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QyE0spvRexi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "\n",
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "\n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implementation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Garde les valeurs nécessaires pour le backwards\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "\n",
    "        # [[STUDENT]] Renvoyer la valeur de la fonction\n",
    "\n",
    "        # [[/STUDENT]]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrées\n",
    "        yhat, y = ctx.saved_tensors\n",
    "        # [[STUDENT]] Renvoyer les deux dérivées partielles (par rapport à yhat et à y)\n",
    "\n",
    "        # [[/STUDENT]]\n",
    "\n",
    "# [[STUDENT]] Implémenter la fonction Linear(X, W, b)sur le même modèle que MSE\n",
    "\n",
    "\n",
    "# [[/STUDENT]]\n",
    "\n",
    "## Utile pour gradcheck\n",
    "mse = MSE.apply\n",
    "linear = Linear.apply\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tlIwKy6Rexj",
    "outputId": "83456655-655a-4f21-a497-d8e873e29ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de MSE\n",
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49Ymm1PERexj",
    "outputId": "96ebc9d5-d541-4cfb-ab15-7350a0763e50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du gradient de Linear (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "w = torch.randn(5, 7,requires_grad=True,dtype=torch.float64)\n",
    "b = torch.randn(7,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(linear,(x,w,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd6RAkfxRexk"
   },
   "source": [
    "## Descente de Gradient\n",
    "\n",
    "### Regression Linéaire\n",
    "\n",
    "Compléter ci-dessous le code pour réaliser la même regression linéaire qu'au TP précédent, mais en utilisant les objets Function déclarés ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcYmY2f1Rexk",
    "outputId": "b8d01166-b684-42b9-e422-1d6e09301156"
   },
   "outputs": [],
   "source": [
    "## Chargement des données California_Housing et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "y = torch.tensor(housing['target'],dtype=torch.float).view(-1,1)\n",
    "\n",
    "print(\"Nombre d'exemples : \",x.size(0), \"Dimension : \",x.size(1))\n",
    "\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(x.size(1),1)\n",
    "b =  torch.randn(1,1)\n",
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-7\n",
    "for n_iter in range(EPOCHS):\n",
    "    ## [[STUDENT]] Calcul du forward (loss), avec creation de nouveaux Context pour chaque module\n",
    "\n",
    "\n",
    "    # `loss` doit correspondre au coût MSE calculé à cette itération\n",
    "    if n_iter % 100==0:\n",
    "        print(f\"Itérations {n_iter}: loss {loss}\")\n",
    "\n",
    "    ## [[STUDENT]] Calcul du backward (grad_w, grad_b)\n",
    "\n",
    "    # [[/STUDENT]]\n",
    "\n",
    "    ## [[STUDENT]] Mise à jour des paramètres du modèle\n",
    "\n",
    "    # [[/STUDENT]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHUnajhRRexk"
   },
   "source": [
    "### Regression Non Linéaire\n",
    "\n",
    "Ajouter une classe Function Tanh sur le modèle des classe déclarées ci-dessus et appliquer une descente de gradient sur le problème précédent qui utilise un réseau de neurones à une couche cachée de 10 neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[STUDENT]] Implémenter la fonction Tanh(X)sur le même modèle que MSE et Linear\n",
    "class Tanh(Function):\n",
    "\n",
    "    \n",
    "# [[/STUDENT]]\n",
    "\n",
    "## Utile pour gradcheck\n",
    "tanh=Tanh.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du gradient de Tanh (sur le même modèle que MSE)\n",
    "\n",
    "x = torch.randn(13, 5,requires_grad=True,dtype=torch.float64)\n",
    "torch.autograd.gradcheck(tanh,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chargement des données California_Housing et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "y = torch.tensor(housing['target'],dtype=torch.float).view(-1,1)\n",
    "\n",
    "print(\"Nombre d'exemples : \",x.size(0), \"Dimension : \",x.size(1))\n",
    "\n",
    "# [[STUDENT]] Implémenter la descente de gradient précédente selon un réseau à une couche cachée de 10 neurones\n",
    "\n",
    "    \n",
    "    \n",
    "# [[/STUDENT]]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
