{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning  \n",
    "\n",
    "\n",
    "## TP3 : Méthodologie, Expérimentations et Régularisation \n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:53:39.268652Z",
     "start_time": "2024-11-19T15:53:39.256253Z"
    }
   },
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "## Chargement des données California_Housing et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "data_x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "data_y = torch.tensor(housing['target'],dtype=torch.float).view(-1)\n",
    "Xdim = data_x.size(1)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.5.1+cu124\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGIDLqItECDu"
   },
   "source": [
    "# Méthodologie expérimentale et boîte à outils\n",
    "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier : \n",
    "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
    "* le checkpointing qui permet de sauvegarder/charger les modèles en cours d'entraînement.\n",
    "* le TensorBoard (qui vient de tensorflow) qui permet de suivre l'évolution en apprentissage de vos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ4MoJP4k4i0"
   },
   "source": [
    "\n",
    "## DataLoader\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader : \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AZaWAFO8k8ze",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:53:39.326117Z",
     "start_time": "2024-11-19T15:53:39.318019Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(data_x,data_y)\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True ) \n",
    "\n",
    "#Premier batch (aléatoire) du dataloader :\n",
    "print(len(iter(loader)),next(iter(loader)))\n",
    "\n",
    "## Exemple d'un Dataset (sans utilité dans le cas présent, TensorDataset permet de faire la même chose)\n",
    "class MyDataSet(Dataset):\n",
    "  def __init__(self, x,y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "  def __getitem__(self,i):\n",
    "    return self.x[i],self.y[i]\n",
    "  def __len__(self):\n",
    "    return len(self.x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 (tensor([   4.0368,   52.0000,    4.7617,    1.1036,  413.0000,    2.1399,\n",
      "          37.8500, -122.2500]), tensor(2.6970))\n",
      "1290 [tensor([[ 2.9000e+00,  3.8000e+01,  5.9474e+00,  1.1579e+00,  1.2500e+02,\n",
      "          3.2895e+00,  3.7900e+01, -1.2131e+02],\n",
      "        [ 4.5156e+00,  1.9000e+01,  5.3545e+00,  1.0346e+00,  7.9000e+02,\n",
      "          2.2767e+00,  3.7250e+01, -1.2196e+02],\n",
      "        [ 2.3381e+00,  1.2000e+01,  3.7880e+00,  1.0035e+00,  1.3200e+03,\n",
      "          2.3322e+00,  3.6820e+01, -1.1971e+02],\n",
      "        [ 3.1600e+00,  1.4000e+01,  3.2049e+00,  1.0531e+00,  1.3190e+03,\n",
      "          2.5028e+00,  3.2690e+01, -1.1705e+02],\n",
      "        [ 5.6233e+00,  2.1000e+01,  6.6051e+00,  1.0412e+00,  2.0140e+03,\n",
      "          2.8608e+00,  3.7030e+01, -1.2203e+02],\n",
      "        [ 2.2372e+00,  1.9000e+01,  4.0525e+00,  1.0131e+00,  2.1430e+03,\n",
      "          3.5131e+00,  3.3880e+01, -1.1755e+02],\n",
      "        [ 4.8611e+00,  2.6000e+01,  5.1766e+00,  1.0136e+00,  1.0980e+03,\n",
      "          2.9837e+00,  3.3710e+01, -1.1798e+02],\n",
      "        [ 1.2157e+00,  3.6000e+01,  5.0246e+00,  1.1834e+00,  1.4900e+03,\n",
      "          2.8166e+00,  3.5360e+01, -1.1901e+02],\n",
      "        [ 7.5925e+00,  3.9000e+01,  7.1505e+00,  9.9769e-01,  1.1750e+03,\n",
      "          2.7199e+00,  3.2720e+01, -1.1724e+02],\n",
      "        [ 3.1103e+00,  4.4000e+01,  4.6618e+00,  9.7977e-01,  7.7600e+02,\n",
      "          2.2428e+00,  3.7710e+01, -1.2213e+02],\n",
      "        [ 1.9727e+00,  2.1000e+01,  5.3669e+00,  1.3604e+00,  7.3700e+02,\n",
      "          2.3929e+00,  3.8900e+01, -1.2000e+02],\n",
      "        [ 4.7344e+00,  3.7000e+01,  5.1361e+00,  9.6073e-01,  1.2670e+03,\n",
      "          3.3168e+00,  3.7560e+01, -1.2230e+02],\n",
      "        [ 6.4664e+00,  2.5000e+01,  6.1923e+00,  8.7870e-01,  9.8300e+02,\n",
      "          2.9083e+00,  3.7310e+01, -1.2178e+02],\n",
      "        [ 5.0346e+00,  1.0000e+01,  7.1066e+00,  1.0761e+00,  2.5730e+03,\n",
      "          3.2652e+00,  3.3880e+01, -1.1736e+02],\n",
      "        [ 7.2731e+00,  4.0000e+00,  8.8614e+00,  1.3438e+00,  5.6130e+03,\n",
      "          3.3096e+00,  3.3020e+01, -1.1715e+02],\n",
      "        [ 2.2264e+00,  1.1000e+01,  2.8994e+00,  1.0526e+00,  2.4660e+03,\n",
      "          2.8842e+00,  3.4190e+01, -1.1845e+02]]), tensor([1.2500, 3.3930, 1.1250, 1.4380, 3.2200, 1.0880, 2.2960, 0.5700, 4.6670,\n",
      "        1.8890, 1.1410, 2.7180, 2.7150, 2.4050, 4.5040, 1.8130])]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:20.566567Z",
     "start_time": "2024-11-19T15:53:39.374947Z"
    }
   },
   "source": [
    "EPOCHS = 100\n",
    "EPS=1e-4\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "# La boucle d'apprentissage :\n",
    "for i in range(EPOCHS):\n",
    "    cumloss = 0\n",
    "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if i % 10==0: print(f\"iteration : {i}, loss : {cumloss/len(loader)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 5.645565078055212\n",
      "iteration : 10, loss : 1.3369016169115555\n",
      "iteration : 20, loss : 1.3346408754356147\n",
      "iteration : 30, loss : 1.3332995466945707\n",
      "iteration : 40, loss : 1.3321710829586946\n",
      "iteration : 50, loss : 1.331578953598821\n",
      "iteration : 60, loss : 1.3311542834884436\n",
      "iteration : 70, loss : 1.3307564742574396\n",
      "iteration : 80, loss : 1.330558390037496\n",
      "iteration : 90, loss : 1.3303420398124428\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9x2LC_6lCQm"
   },
   "source": [
    "## Checkpointing\n",
    "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
    "\n",
    "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "URQTq8hrPJO0",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:20.580234Z",
     "start_time": "2024-11-19T15:54:20.576556Z"
    }
   },
   "source": [
    "import os\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
    "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "      torch.save(state,fichier)\n",
    " \n",
    "def load_state(fichier,model,optim):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
    "      epoch = 0\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          optim.load_state_dict(state['optim_state'])\n",
    "          epoch = state['epoch']\n",
    "      return epoch\n",
    "\n",
    "fichier = \"./tmp/netSeq.pth\" \n",
    "save_state(EPOCHS,netSeq,optim,fichier)    \n",
    "    "
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:55:06.366447Z",
     "start_time": "2024-11-19T15:54:20.623246Z"
    }
   },
   "source": [
    "# On crée un autre réseau similaire à celui sauvegardé et on charge les poids \n",
    "# on peut observer qu'on repart du même point que le réseau précédent\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "start_epoch = load_state(fichier,netSeq,optim)\n",
    "for epoch in range(start_epoch,start_epoch+EPOCHS):\n",
    "    cumloss = 0\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if epoch % 10 ==0: \n",
    "        save_state(epoch,netSeq,optim,fichier)\n",
    "        print(f\"epoch : {epoch}, loss : {cumloss/len(loader)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115050/1912322656.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(fichier)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100, loss : 1.3301140796075495\n",
      "epoch : 110, loss : 1.3299213501141054\n",
      "epoch : 120, loss : 1.3296866832084433\n",
      "epoch : 130, loss : 1.329072825830112\n",
      "epoch : 140, loss : 1.3285200387239455\n",
      "epoch : 150, loss : 1.3282680621211842\n",
      "epoch : 160, loss : 1.3280524341866027\n",
      "epoch : 170, loss : 1.3278753649703292\n",
      "epoch : 180, loss : 1.3278604315463887\n",
      "epoch : 190, loss : 1.3277944235145585\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IstQCvKblSvT"
   },
   "source": [
    "\n",
    "## GPU \n",
    "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
    "\n",
    "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
    "\n",
    "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fs8s7EwwlWTn",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:55:06.816895Z",
     "start_time": "2024-11-19T15:55:06.378283Z"
    }
   },
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "loader = DataLoader(TensorDataset(data_x,data_y), batch_size=16,shuffle=True ) \n",
    "\n",
    "## On charge le modèle sur GPU\n",
    "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
    "\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netSeq = netSeq.to(device)\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "for i,(bx,by) in enumerate(loader):\n",
    "    ## On charge le batch sur GPU\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    loss = mseloss(netSeq(bx).view(-1),by)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "print(\"Device du mini-batch : \", bx.device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device du mini-batch :  cpu\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5J1b55_lFR-"
   },
   "source": [
    "\n",
    "## TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.tensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1kIhHDnElQd8",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:56:33.789014Z",
     "start_time": "2024-11-19T15:55:06.837300Z"
    }
   },
   "source": [
    "# Pour observer les courbes produites, il faut lancer tensorboard \n",
    "# à la main à partir du shell :  tensorboard --logdir /tmp/logs/deep\n",
    "TB_PATH = \"/tmp/logs/deep\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class DeuxCouches(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeuxCouches,self).__init__()\n",
    "    self.un = torch.nn.Linear(Xdim,5)\n",
    "    self.act = torch.nn.Tanh()\n",
    "    self.deux = torch.nn.Linear(5,1)\n",
    "  def forward(self,x):\n",
    "    return self.deux(self.act(self.un(x)))\n",
    "\n",
    "EPS = 1e-5\n",
    "EPOCHS=100\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netDeuxCouches = DeuxCouches()\n",
    "netSeq.name = \"Sequentiel\"\n",
    "netDeuxCouches.name = \"DeuxCouches\"\n",
    "## Obtention d'un SummaryWriter \n",
    "summary = SummaryWriter(f\"{TB_PATH}/test\")\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for model in [netSeq, netDeuxCouches]:\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=EPS) \n",
    "    for i in range(EPOCHS):\n",
    "        cumloss = 0\n",
    "        for boston_x, boston_y in loader:\n",
    "            loss = mseloss(model(boston_x),boston_y.view(-1,1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "            cumloss+= loss.item()\n",
    "        summary.add_scalar(f\"loss/{model.name}\",cumloss/len(loader),i)\n",
    "        print(f\"epoch : {i}, loss : {cumloss/len(loader)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 2.7780328563941543\n",
      "epoch : 1, loss : 2.609091105756834\n",
      "epoch : 2, loss : 2.4531986987636993\n",
      "epoch : 3, loss : 2.310349767637807\n",
      "epoch : 4, loss : 2.178908859366594\n",
      "epoch : 5, loss : 2.0587534875832785\n",
      "epoch : 6, loss : 1.9489000578374827\n",
      "epoch : 7, loss : 1.8495408533964046\n",
      "epoch : 8, loss : 1.760748420805894\n",
      "epoch : 9, loss : 1.68203183601993\n",
      "epoch : 10, loss : 1.613414721699186\n",
      "epoch : 11, loss : 1.554817347829194\n",
      "epoch : 12, loss : 1.5058231778154076\n",
      "epoch : 13, loss : 1.4659322890546895\n",
      "epoch : 14, loss : 1.4340372539305872\n",
      "epoch : 15, loss : 1.4093318555244179\n",
      "epoch : 16, loss : 1.3910551376814067\n",
      "epoch : 17, loss : 1.3777964289798292\n",
      "epoch : 18, loss : 1.3685564531143322\n",
      "epoch : 19, loss : 1.3621795910966488\n",
      "epoch : 20, loss : 1.3578026536592218\n",
      "epoch : 21, loss : 1.3548197880271793\n",
      "epoch : 22, loss : 1.3526834030253019\n",
      "epoch : 23, loss : 1.351118807875833\n",
      "epoch : 24, loss : 1.3499981636455818\n",
      "epoch : 25, loss : 1.3491954716716625\n",
      "epoch : 26, loss : 1.3486213254143102\n",
      "epoch : 27, loss : 1.348182796761971\n",
      "epoch : 28, loss : 1.3478204774302105\n",
      "epoch : 29, loss : 1.3475280087585597\n",
      "epoch : 30, loss : 1.347274906621423\n",
      "epoch : 31, loss : 1.3470320806484815\n",
      "epoch : 32, loss : 1.3467930746170902\n",
      "epoch : 33, loss : 1.3465536660002184\n",
      "epoch : 34, loss : 1.3463097306878067\n",
      "epoch : 35, loss : 1.3460536452919938\n",
      "epoch : 36, loss : 1.3458010317743287\n",
      "epoch : 37, loss : 1.3455377981182217\n",
      "epoch : 38, loss : 1.3452862674644752\n",
      "epoch : 39, loss : 1.3450376710457395\n",
      "epoch : 40, loss : 1.3447702026875443\n",
      "epoch : 41, loss : 1.3445310507402863\n",
      "epoch : 42, loss : 1.3442855419114579\n",
      "epoch : 43, loss : 1.3440430918636248\n",
      "epoch : 44, loss : 1.3438010455795038\n",
      "epoch : 45, loss : 1.3435650357442308\n",
      "epoch : 46, loss : 1.3433329384571822\n",
      "epoch : 47, loss : 1.3431205882582553\n",
      "epoch : 48, loss : 1.342905346552531\n",
      "epoch : 49, loss : 1.3426937647799189\n",
      "epoch : 50, loss : 1.3424944006888442\n",
      "epoch : 51, loss : 1.34228167950876\n",
      "epoch : 52, loss : 1.3420714760704557\n",
      "epoch : 53, loss : 1.3418509563272314\n",
      "epoch : 54, loss : 1.3416408143648806\n",
      "epoch : 55, loss : 1.341432416462159\n",
      "epoch : 56, loss : 1.3412212937839272\n",
      "epoch : 57, loss : 1.34102907448776\n",
      "epoch : 58, loss : 1.3408532084998235\n",
      "epoch : 59, loss : 1.3406967424607092\n",
      "epoch : 60, loss : 1.3405449127735094\n",
      "epoch : 61, loss : 1.340400294509045\n",
      "epoch : 62, loss : 1.3402540224005086\n",
      "epoch : 63, loss : 1.3401136949773906\n",
      "epoch : 64, loss : 1.3399741474279137\n",
      "epoch : 65, loss : 1.3398258634084879\n",
      "epoch : 66, loss : 1.3396777568168419\n",
      "epoch : 67, loss : 1.3395319198222124\n",
      "epoch : 68, loss : 1.3393833549909813\n",
      "epoch : 69, loss : 1.3392458563858225\n",
      "epoch : 70, loss : 1.3391143993236299\n",
      "epoch : 71, loss : 1.3390055371347325\n",
      "epoch : 72, loss : 1.3389058141052261\n",
      "epoch : 73, loss : 1.338812157188275\n",
      "epoch : 74, loss : 1.3387358643749887\n",
      "epoch : 75, loss : 1.3386666718148446\n",
      "epoch : 76, loss : 1.3386058155418366\n",
      "epoch : 77, loss : 1.338551953227021\n",
      "epoch : 78, loss : 1.3384826964879222\n",
      "epoch : 79, loss : 1.3384218534873438\n",
      "epoch : 80, loss : 1.3383504762205967\n",
      "epoch : 81, loss : 1.3382807587468346\n",
      "epoch : 82, loss : 1.3381917715534684\n",
      "epoch : 83, loss : 1.3381037607211475\n",
      "epoch : 84, loss : 1.338002456851708\n",
      "epoch : 85, loss : 1.3378876215727755\n",
      "epoch : 86, loss : 1.337769683571749\n",
      "epoch : 87, loss : 1.3376401695170144\n",
      "epoch : 88, loss : 1.3375069568785587\n",
      "epoch : 89, loss : 1.3373735599739607\n",
      "epoch : 90, loss : 1.3372529873783274\n",
      "epoch : 91, loss : 1.3371488250965295\n",
      "epoch : 92, loss : 1.337063978855\n",
      "epoch : 93, loss : 1.3369945011859716\n",
      "epoch : 94, loss : 1.3369362109737803\n",
      "epoch : 95, loss : 1.3368855171425398\n",
      "epoch : 96, loss : 1.3368451508209687\n",
      "epoch : 97, loss : 1.3368026658546093\n",
      "epoch : 98, loss : 1.336765310399292\n",
      "epoch : 99, loss : 1.336728092288786\n",
      "epoch : 0, loss : 6.295756635555001\n",
      "epoch : 1, loss : 5.966336384300114\n",
      "epoch : 2, loss : 5.651228466144828\n",
      "epoch : 3, loss : 5.348490824163422\n",
      "epoch : 4, loss : 5.057399174224499\n",
      "epoch : 5, loss : 4.776930503142896\n",
      "epoch : 6, loss : 4.507671678158664\n",
      "epoch : 7, loss : 4.251210704145506\n",
      "epoch : 8, loss : 4.006715715208719\n",
      "epoch : 9, loss : 3.773713671468025\n",
      "epoch : 10, loss : 3.5518554199111554\n",
      "epoch : 11, loss : 3.3419863687467206\n",
      "epoch : 12, loss : 3.143431460996007\n",
      "epoch : 13, loss : 2.9556073391622353\n",
      "epoch : 14, loss : 2.7789268273715826\n",
      "epoch : 15, loss : 2.6137467457573544\n",
      "epoch : 16, loss : 2.4597212591605593\n",
      "epoch : 17, loss : 2.316590608032637\n",
      "epoch : 18, loss : 2.184297972524813\n",
      "epoch : 19, loss : 2.062732679783836\n",
      "epoch : 20, loss : 1.9519186549870542\n",
      "epoch : 21, loss : 1.8517418341696725\n",
      "epoch : 22, loss : 1.762176080798918\n",
      "epoch : 23, loss : 1.6825356633395188\n",
      "epoch : 24, loss : 1.6133175500719121\n",
      "epoch : 25, loss : 1.553753933241201\n",
      "epoch : 26, loss : 1.5033425440737442\n",
      "epoch : 27, loss : 1.461523257369219\n",
      "epoch : 28, loss : 1.4279359148338784\n",
      "epoch : 29, loss : 1.401619012861751\n",
      "epoch : 30, loss : 1.3819088669017303\n",
      "epoch : 31, loss : 1.3678923979055049\n",
      "epoch : 32, loss : 1.3580548048943513\n",
      "epoch : 33, loss : 1.351461370300877\n",
      "epoch : 34, loss : 1.3472026489501776\n",
      "epoch : 35, loss : 1.3445238692584889\n",
      "epoch : 36, loss : 1.342819674463235\n",
      "epoch : 37, loss : 1.3417701558087223\n",
      "epoch : 38, loss : 1.3411298139959342\n",
      "epoch : 39, loss : 1.3407491955877275\n",
      "epoch : 40, loss : 1.3405029810214228\n",
      "epoch : 41, loss : 1.3403477496532508\n",
      "epoch : 42, loss : 1.3402505509151044\n",
      "epoch : 43, loss : 1.3401813154996827\n",
      "epoch : 44, loss : 1.340130268788153\n",
      "epoch : 45, loss : 1.3400883773269578\n",
      "epoch : 46, loss : 1.3400544707396234\n",
      "epoch : 47, loss : 1.3400249888961628\n",
      "epoch : 48, loss : 1.3400008113578308\n",
      "epoch : 49, loss : 1.3399762750827064\n",
      "epoch : 50, loss : 1.3399471514100252\n",
      "epoch : 51, loss : 1.339922951543054\n",
      "epoch : 52, loss : 1.3399014979138855\n",
      "epoch : 53, loss : 1.3398799795513006\n",
      "epoch : 54, loss : 1.3398566650558812\n",
      "epoch : 55, loss : 1.3398315723321235\n",
      "epoch : 56, loss : 1.3398145774076151\n",
      "epoch : 57, loss : 1.3397875617178836\n",
      "epoch : 58, loss : 1.3397687047950981\n",
      "epoch : 59, loss : 1.3397420755421467\n",
      "epoch : 60, loss : 1.3397240145492924\n",
      "epoch : 61, loss : 1.3397025985542195\n",
      "epoch : 62, loss : 1.339681417535442\n",
      "epoch : 63, loss : 1.339653931708299\n",
      "epoch : 64, loss : 1.3396390268275904\n",
      "epoch : 65, loss : 1.3396166371051654\n",
      "epoch : 66, loss : 1.3395966782126316\n",
      "epoch : 67, loss : 1.339572062441545\n",
      "epoch : 68, loss : 1.339552352155826\n",
      "epoch : 69, loss : 1.3395277511241823\n",
      "epoch : 70, loss : 1.3395107260392618\n",
      "epoch : 71, loss : 1.339487352389698\n",
      "epoch : 72, loss : 1.3394700691912524\n",
      "epoch : 73, loss : 1.3394486207139584\n",
      "epoch : 74, loss : 1.3394254799037018\n",
      "epoch : 75, loss : 1.3394118098672045\n",
      "epoch : 76, loss : 1.3393908977508544\n",
      "epoch : 77, loss : 1.3393687664769416\n",
      "epoch : 78, loss : 1.3393458984387938\n",
      "epoch : 79, loss : 1.3393276964971261\n",
      "epoch : 80, loss : 1.3393089050932447\n",
      "epoch : 81, loss : 1.339288736175197\n",
      "epoch : 82, loss : 1.3392697873503663\n",
      "epoch : 83, loss : 1.3392474824837013\n",
      "epoch : 84, loss : 1.3392294039444406\n",
      "epoch : 85, loss : 1.3392056909642478\n",
      "epoch : 86, loss : 1.3391884639974712\n",
      "epoch : 87, loss : 1.339169659032378\n",
      "epoch : 88, loss : 1.33915347285973\n",
      "epoch : 89, loss : 1.3391261401795602\n",
      "epoch : 90, loss : 1.3391113972594573\n",
      "epoch : 91, loss : 1.3390937595866448\n",
      "epoch : 92, loss : 1.339073015252749\n",
      "epoch : 93, loss : 1.3390536514132523\n",
      "epoch : 94, loss : 1.3390337242867596\n",
      "epoch : 95, loss : 1.3390124293491823\n",
      "epoch : 96, loss : 1.3389955497411794\n",
      "epoch : 97, loss : 1.338975284935892\n",
      "epoch : 98, loss : 1.338961626947388\n",
      "epoch : 99, loss : 1.338941297558851\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualisation des courbes dans tensorboard\n",
    "![Capture du tensorboard](./img/tensorBoard1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "## Dernières remarques et exemple typique de code\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:56:33.800499Z",
     "start_time": "2024-11-19T15:56:33.797882Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "TB_PATH = \"/tmp/logs/module1\"\n",
    "MODEL_PATH = \"/tmp/models\"\n",
    "os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b3TRg2p5ldCJ",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:56:33.849176Z",
     "start_time": "2024-11-19T15:56:33.844292Z"
    }
   },
   "source": [
    "def save_state(fichier,epoch,model,optim):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train(model, loss, epochs, train_loader, test_loader,lr=1e-3):\n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}.pth\"\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=lr)\n",
    "    start_epoch = load_state(check_file,model,optim)\n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        # Apprentissage\n",
    "        # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        for xbatch, ybatch in train_loader:\n",
    "            xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "            outputs = model(xbatch)\n",
    "            l = loss(outputs.view(-1),ybatch)\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l.item()\n",
    "        summary.add_scalar(\"loss/train\",  cumloss/len(train_loader),epoch)\n",
    "        print(f\"epoch : {epoch}, loss : {cumloss/len(train_loader)}\")\n",
    "        \n",
    "        if epoch % 10 == 0: \n",
    "            save_state(check_file,epoch,model,optim)\n",
    "            # Validation\n",
    "            # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss = 0\n",
    "                for xbatch, ybatch in test_loader:\n",
    "                    xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                    outputs = model(xbatch)\n",
    "                    cumloss += loss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation\", cumloss/len(test_loader) ,epoch)\n",
    "            print(f\"epoch validation: {epoch}, loss : {cumloss/len(test_loader)}\")\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-19T15:56:33.899861Z"
    }
   },
   "source": [
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "all_data = torch.tensor(housing['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(housing['target'],dtype=torch.float).view(-1)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau\"+time.asctime()\n",
    "\n",
    "net = net.to(device)\n",
    "train(net,torch.nn.MSELoss(),EPOCHS,train_loader,valid_loader,lr=1e-5)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 0.9486120659136033\n",
      "epoch validation: 0, loss : 0.9366910186155822\n",
      "epoch : 1, loss : 0.9181362042246863\n",
      "epoch : 2, loss : 0.8885288084026917\n",
      "epoch : 3, loss : 0.8597702619930109\n",
      "epoch : 4, loss : 0.8317521028653827\n",
      "epoch : 5, loss : 0.804733335538659\n",
      "epoch : 6, loss : 0.7784033043295607\n",
      "epoch : 7, loss : 0.752984587175324\n",
      "epoch : 8, loss : 0.728544780479152\n",
      "epoch : 9, loss : 0.7048921460473492\n",
      "epoch : 10, loss : 0.6821700935360304\n",
      "epoch validation: 10, loss : 0.6698730829381203\n",
      "epoch : 11, loss : 0.6605668403930212\n",
      "epoch : 12, loss : 0.6399964591444924\n",
      "epoch : 13, loss : 0.6204426999586498\n",
      "epoch : 14, loss : 0.6019796064744393\n",
      "epoch : 15, loss : 0.5846647436824418\n",
      "epoch : 16, loss : 0.5684158118485019\n",
      "epoch : 17, loss : 0.5531927381403917\n",
      "epoch : 18, loss : 0.5389984333382327\n",
      "epoch : 19, loss : 0.5258107275175031\n",
      "epoch : 20, loss : 0.5136124225434406\n",
      "epoch validation: 20, loss : 0.49885085150021913\n",
      "epoch : 21, loss : 0.5024036373365526\n",
      "epoch : 22, loss : 0.4921495482585458\n",
      "epoch : 23, loss : 0.4828209512859814\n",
      "epoch : 24, loss : 0.474268123081546\n",
      "epoch : 25, loss : 0.4665519631532736\n",
      "epoch : 26, loss : 0.4596509383558187\n",
      "epoch : 27, loss : 0.4534296726143753\n",
      "epoch : 28, loss : 0.44781821825389945\n",
      "epoch : 29, loss : 0.4427582731383831\n",
      "epoch : 30, loss : 0.43813043151252956\n",
      "epoch validation: 30, loss : 0.4196812119190545\n",
      "epoch : 31, loss : 0.43393384867398316\n",
      "epoch : 32, loss : 0.4301349222226891\n",
      "epoch : 33, loss : 0.42667360598755666\n",
      "epoch : 34, loss : 0.42349298945952984\n",
      "epoch : 35, loss : 0.42057122765903093\n",
      "epoch : 36, loss : 0.41786179126261974\n",
      "epoch : 37, loss : 0.41533502048357973\n",
      "epoch : 38, loss : 0.41294489784325045\n",
      "epoch : 39, loss : 0.41068599501610265\n",
      "epoch : 40, loss : 0.4085530912099305\n",
      "epoch validation: 40, loss : 0.3873364377853482\n",
      "epoch : 41, loss : 0.4065107688842009\n",
      "epoch : 42, loss : 0.4045483419498385\n",
      "epoch : 43, loss : 0.40267331537956647\n",
      "epoch : 44, loss : 0.4008819253884198\n",
      "epoch : 45, loss : 0.3991537828417134\n",
      "epoch : 46, loss : 0.3974720682631167\n",
      "epoch : 47, loss : 0.39583451103765604\n",
      "epoch : 48, loss : 0.39424992638618445\n",
      "epoch : 49, loss : 0.39272074093103637\n",
      "epoch : 50, loss : 0.3912282203605702\n",
      "epoch validation: 50, loss : 0.3691228811535262\n",
      "epoch : 51, loss : 0.3897796836068002\n",
      "epoch : 52, loss : 0.3883738186744641\n",
      "epoch : 53, loss : 0.3870017797895527\n",
      "epoch : 54, loss : 0.385670682855759\n",
      "epoch : 55, loss : 0.384371185097699\n",
      "epoch : 56, loss : 0.3831026192732848\n",
      "epoch : 57, loss : 0.38186537550921124\n",
      "epoch : 58, loss : 0.3806625411289838\n",
      "epoch : 59, loss : 0.37948547346271977\n",
      "epoch : 60, loss : 0.37833190782991954\n",
      "epoch validation: 60, loss : 0.35636004438811497\n",
      "epoch : 61, loss : 0.37720415587428696\n",
      "epoch : 62, loss : 0.37611336214106905\n",
      "epoch : 63, loss : 0.3750458620633035\n",
      "epoch : 64, loss : 0.3740039641044803\n",
      "epoch : 65, loss : 0.37299216479828307\n",
      "epoch : 66, loss : 0.37199770039048535\n",
      "epoch : 67, loss : 0.3710303124141439\n",
      "epoch : 68, loss : 0.3700765471241271\n",
      "epoch : 69, loss : 0.3691537693186208\n",
      "epoch : 70, loss : 0.3682494271986995\n",
      "epoch validation: 70, loss : 0.3468024887202322\n",
      "epoch : 71, loss : 0.3673625596791737\n",
      "epoch : 72, loss : 0.36650692911768085\n",
      "epoch : 73, loss : 0.3656682505799356\n",
      "epoch : 74, loss : 0.3648440587359603\n",
      "epoch : 75, loss : 0.36404329345222247\n",
      "epoch : 76, loss : 0.3632598290862443\n",
      "epoch : 77, loss : 0.3624896131087701\n",
      "epoch : 78, loss : 0.3617528501190534\n",
      "epoch : 79, loss : 0.36102815150001716\n",
      "epoch : 80, loss : 0.36032372033728877\n",
      "epoch validation: 80, loss : 0.33951099346890007\n",
      "epoch : 81, loss : 0.35964266926613314\n",
      "epoch : 82, loss : 0.3589669671415012\n",
      "epoch : 83, loss : 0.35831832951079157\n",
      "epoch : 84, loss : 0.3576806327797357\n",
      "epoch : 85, loss : 0.3570672043440358\n",
      "epoch : 86, loss : 0.3564660242513226\n",
      "epoch : 87, loss : 0.3558722154576649\n",
      "epoch : 88, loss : 0.35529193936941006\n",
      "epoch : 89, loss : 0.35473168658655746\n",
      "epoch : 90, loss : 0.35418604535967574\n",
      "epoch validation: 90, loss : 0.33398056578959606\n",
      "epoch : 91, loss : 0.3536526798270643\n",
      "epoch : 92, loss : 0.35312919123748016\n",
      "epoch : 93, loss : 0.35261358110623997\n",
      "epoch : 94, loss : 0.3521132387487983\n",
      "epoch : 95, loss : 0.3516271448217679\n",
      "epoch : 96, loss : 0.3511486271822233\n",
      "epoch : 97, loss : 0.350679775198214\n",
      "epoch : 98, loss : 0.3502230731005123\n",
      "epoch : 99, loss : 0.34977900577968984\n",
      "epoch : 100, loss : 0.34934533605453116\n",
      "epoch validation: 100, loss : 0.3296944536268711\n",
      "epoch : 101, loss : 0.3489180141374065\n",
      "epoch : 102, loss : 0.34850264537011005\n",
      "epoch : 103, loss : 0.3481027260133924\n",
      "epoch : 104, loss : 0.3477078527717512\n",
      "epoch : 105, loss : 0.34731954209824173\n",
      "epoch : 106, loss : 0.34694138647223166\n",
      "epoch : 107, loss : 0.3465733119510403\n",
      "epoch : 108, loss : 0.34620994329091537\n",
      "epoch : 109, loss : 0.34585497420382244\n",
      "epoch : 110, loss : 0.34550430266865234\n",
      "epoch validation: 110, loss : 0.3263484909552936\n",
      "epoch : 111, loss : 0.3451662689570135\n",
      "epoch : 112, loss : 0.3448302131073997\n",
      "epoch : 113, loss : 0.3444995038254663\n",
      "epoch : 114, loss : 0.34418026552456177\n",
      "epoch : 115, loss : 0.3438625056440978\n",
      "epoch : 116, loss : 0.3435606678384681\n",
      "epoch : 117, loss : 0.3432544266494612\n",
      "epoch : 118, loss : 0.3429586389787899\n",
      "epoch : 119, loss : 0.3426632406334429\n",
      "epoch : 120, loss : 0.3423776694559023\n",
      "epoch validation: 120, loss : 0.3236657841665338\n",
      "epoch : 121, loss : 0.34209820072621455\n",
      "epoch : 122, loss : 0.34181840374502676\n",
      "epoch : 123, loss : 0.34154343895628586\n",
      "epoch : 124, loss : 0.3412772700327716\n",
      "epoch : 125, loss : 0.34101302376695614\n",
      "epoch : 126, loss : 0.3407494200956683\n",
      "epoch : 127, loss : 0.3404929312054328\n",
      "epoch : 128, loss : 0.3402425399259484\n",
      "epoch : 129, loss : 0.33999397393402664\n",
      "epoch : 130, loss : 0.3397506617962621\n",
      "epoch validation: 130, loss : 0.3214375633717507\n",
      "epoch : 131, loss : 0.33950692491764706\n",
      "epoch : 132, loss : 0.3392692489058588\n",
      "epoch : 133, loss : 0.339039308462985\n",
      "epoch : 134, loss : 0.338811654951279\n",
      "epoch : 135, loss : 0.3385848929607418\n",
      "epoch : 136, loss : 0.33836142695689386\n",
      "epoch : 137, loss : 0.3381425581923\n",
      "epoch : 138, loss : 0.33792756101506394\n",
      "epoch : 139, loss : 0.3377161823184222\n",
      "epoch : 140, loss : 0.3375039230707253\n",
      "epoch validation: 140, loss : 0.31955348905320313\n",
      "epoch : 141, loss : 0.33729800595649223\n",
      "epoch : 142, loss : 0.33709297519422665\n",
      "epoch : 143, loss : 0.33689271076401883\n",
      "epoch : 144, loss : 0.3366913527192533\n",
      "epoch : 145, loss : 0.336491256461356\n",
      "epoch : 146, loss : 0.33629733280733576\n",
      "epoch : 147, loss : 0.336102840396785\n",
      "epoch : 148, loss : 0.3359156101472791\n",
      "epoch : 149, loss : 0.33572753063626876\n",
      "epoch : 150, loss : 0.33554849183085816\n",
      "epoch validation: 150, loss : 0.31791672779088337\n",
      "epoch : 151, loss : 0.33536395752918\n",
      "epoch : 152, loss : 0.3351786623947149\n",
      "epoch : 153, loss : 0.33500135835086886\n",
      "epoch : 154, loss : 0.33482761656894355\n",
      "epoch : 155, loss : 0.33464991717866455\n",
      "epoch : 156, loss : 0.3344804171234543\n",
      "epoch : 157, loss : 0.33431281380052136\n",
      "epoch : 158, loss : 0.33414432269253935\n",
      "epoch : 159, loss : 0.3339814128852341\n",
      "epoch : 160, loss : 0.3338100740130153\n",
      "epoch validation: 160, loss : 0.31645832548654357\n",
      "epoch : 161, loss : 0.33364910963388494\n",
      "epoch : 162, loss : 0.3334849347512043\n",
      "epoch : 163, loss : 0.3333273334573059\n",
      "epoch : 164, loss : 0.33316745014514687\n",
      "epoch : 165, loss : 0.33301150141818114\n",
      "epoch : 166, loss : 0.33285869346830504\n",
      "epoch : 167, loss : 0.33270862583042116\n",
      "epoch : 168, loss : 0.33255901898423484\n",
      "epoch : 169, loss : 0.3324130212657096\n",
      "epoch : 170, loss : 0.33226514664702406\n",
      "epoch validation: 170, loss : 0.3151510877664699\n",
      "epoch : 171, loss : 0.3321183303934197\n",
      "epoch : 172, loss : 0.3319684098647951\n",
      "epoch : 173, loss : 0.33182602667776767\n",
      "epoch : 174, loss : 0.331686378864557\n",
      "epoch : 175, loss : 0.3315431683959598\n",
      "epoch : 176, loss : 0.3314020587275772\n",
      "epoch : 177, loss : 0.3312604263381556\n",
      "epoch : 178, loss : 0.33112189405446135\n",
      "epoch : 179, loss : 0.3309861178899747\n",
      "epoch : 180, loss : 0.3308508606670901\n",
      "epoch validation: 180, loss : 0.3139612295686506\n",
      "epoch : 181, loss : 0.33071711391224995\n",
      "epoch : 182, loss : 0.3305871912798043\n",
      "epoch : 183, loss : 0.33045120243084175\n",
      "epoch : 184, loss : 0.33031884272575607\n",
      "epoch : 185, loss : 0.33019038806705514\n",
      "epoch : 186, loss : 0.33005967522906354\n",
      "epoch : 187, loss : 0.32993327579352744\n",
      "epoch : 188, loss : 0.3298060870476702\n",
      "epoch : 189, loss : 0.32968085813299974\n",
      "epoch : 190, loss : 0.32955667947946016\n",
      "epoch validation: 190, loss : 0.3128624834091395\n",
      "epoch : 191, loss : 0.3294298887765396\n",
      "epoch : 192, loss : 0.3293085647891312\n",
      "epoch : 193, loss : 0.3291876830644328\n",
      "epoch : 194, loss : 0.329070223109118\n",
      "epoch : 195, loss : 0.32895082470332815\n",
      "epoch : 196, loss : 0.3288324943584816\n",
      "epoch : 197, loss : 0.32871155830663307\n",
      "epoch : 198, loss : 0.32859684118293514\n",
      "epoch : 199, loss : 0.32847749764576206\n",
      "epoch : 200, loss : 0.3283625799989284\n",
      "epoch validation: 200, loss : 0.31183797558552995\n",
      "epoch : 201, loss : 0.3282494447302333\n",
      "epoch : 202, loss : 0.3281365526305844\n",
      "epoch : 203, loss : 0.32802542453891664\n",
      "epoch : 204, loss : 0.3279125554118912\n",
      "epoch : 205, loss : 0.32780690094484954\n",
      "epoch : 206, loss : 0.3276934512898095\n",
      "epoch : 207, loss : 0.3275861177275809\n",
      "epoch : 208, loss : 0.32747746161149105\n",
      "epoch : 209, loss : 0.32737028065621276\n",
      "epoch : 210, loss : 0.32726283532805567\n",
      "epoch validation: 210, loss : 0.3109024083314016\n",
      "epoch : 211, loss : 0.3271575179492492\n",
      "epoch : 212, loss : 0.3270527667243236\n",
      "epoch : 213, loss : 0.3269487641383569\n",
      "epoch : 214, loss : 0.32684319579338494\n",
      "epoch : 215, loss : 0.32673976140082345\n",
      "epoch : 216, loss : 0.32663969419420114\n",
      "epoch : 217, loss : 0.32654034233717033\n",
      "epoch : 218, loss : 0.3264387756614953\n",
      "epoch : 219, loss : 0.3263376947463483\n",
      "epoch : 220, loss : 0.3262449030605571\n",
      "epoch validation: 220, loss : 0.31001972734870376\n",
      "epoch : 221, loss : 0.3261422382147853\n",
      "epoch : 222, loss : 0.3260421997600401\n",
      "epoch : 223, loss : 0.325950171711833\n",
      "epoch : 224, loss : 0.3258558933139425\n",
      "epoch : 225, loss : 0.3257589604481766\n",
      "epoch : 226, loss : 0.32566466792883686\n",
      "epoch : 227, loss : 0.32556827348303197\n",
      "epoch : 228, loss : 0.3254783390479725\n",
      "epoch : 229, loss : 0.3253856632540854\n",
      "epoch : 230, loss : 0.32529502462003695\n",
      "epoch validation: 230, loss : 0.3091827587246202\n",
      "epoch : 231, loss : 0.325202811993081\n",
      "epoch : 232, loss : 0.3251154191878646\n",
      "epoch : 233, loss : 0.3250268546200191\n",
      "epoch : 234, loss : 0.32493418064545987\n",
      "epoch : 235, loss : 0.32484534832237416\n",
      "epoch : 236, loss : 0.3247575931785296\n",
      "epoch : 237, loss : 0.32466962803588356\n",
      "epoch : 238, loss : 0.3245908444812305\n",
      "epoch : 239, loss : 0.32450044228399333\n",
      "epoch : 240, loss : 0.3244155610897109\n",
      "epoch validation: 240, loss : 0.30841070261978826\n",
      "epoch : 241, loss : 0.324331022459696\n",
      "epoch : 242, loss : 0.32424523325117177\n",
      "epoch : 243, loss : 0.3241619995593043\n",
      "epoch : 244, loss : 0.3240766664930843\n",
      "epoch : 245, loss : 0.3239951618340646\n",
      "epoch : 246, loss : 0.3239155452438565\n",
      "epoch : 247, loss : 0.32383063203806794\n",
      "epoch : 248, loss : 0.3237496582148669\n",
      "epoch : 249, loss : 0.3236718870908773\n",
      "epoch : 250, loss : 0.3235911048313444\n",
      "epoch validation: 250, loss : 0.30768835460839344\n",
      "epoch : 251, loss : 0.3235105378986445\n",
      "epoch : 252, loss : 0.32343519062828996\n",
      "epoch : 253, loss : 0.3233573302814318\n",
      "epoch : 254, loss : 0.3232767306224898\n",
      "epoch : 255, loss : 0.3231995435135598\n",
      "epoch : 256, loss : 0.32312558202592784\n",
      "epoch : 257, loss : 0.3230484649827776\n",
      "epoch : 258, loss : 0.3229709447780263\n",
      "epoch : 259, loss : 0.32289827755443\n",
      "epoch : 260, loss : 0.3228231798767206\n",
      "epoch validation: 260, loss : 0.30701571691405866\n",
      "epoch : 261, loss : 0.3227476378101422\n",
      "epoch : 262, loss : 0.3226737360029604\n",
      "epoch : 263, loss : 0.3225987981824392\n",
      "epoch : 264, loss : 0.3225264486300853\n",
      "epoch : 265, loss : 0.32245113876063464\n",
      "epoch : 266, loss : 0.3223812830446707\n",
      "epoch : 267, loss : 0.32230725744378197\n",
      "epoch : 268, loss : 0.32223706281664527\n",
      "epoch : 269, loss : 0.3221652847570331\n",
      "epoch : 270, loss : 0.3220962628996534\n",
      "epoch validation: 270, loss : 0.3063696823696526\n",
      "epoch : 271, loss : 0.3220243004598301\n",
      "epoch : 272, loss : 0.32195449905657725\n",
      "epoch : 273, loss : 0.3218845290767188\n",
      "epoch : 274, loss : 0.3218142733288308\n",
      "epoch : 275, loss : 0.3217466863832444\n",
      "epoch : 276, loss : 0.321677529538238\n",
      "epoch : 277, loss : 0.32160702773709166\n",
      "epoch : 278, loss : 0.3215433450473487\n",
      "epoch : 279, loss : 0.3214762490964675\n",
      "epoch : 280, loss : 0.3214110421132325\n",
      "epoch validation: 280, loss : 0.3057583210368951\n",
      "epoch : 281, loss : 0.32134365540921916\n",
      "epoch : 282, loss : 0.32127791753962864\n",
      "epoch : 283, loss : 0.3212128023999606\n",
      "epoch : 284, loss : 0.321145841324803\n",
      "epoch : 285, loss : 0.3210805010830247\n",
      "epoch : 286, loss : 0.32101573575966696\n",
      "epoch : 287, loss : 0.3209551376463889\n",
      "epoch : 288, loss : 0.320888471190435\n",
      "epoch : 289, loss : 0.3208279271083054\n",
      "epoch : 290, loss : 0.3207573064335788\n",
      "epoch validation: 290, loss : 0.30517470329826657\n",
      "epoch : 291, loss : 0.3206951583576815\n",
      "epoch : 292, loss : 0.32063085519051715\n",
      "epoch : 293, loss : 0.32057004438915343\n",
      "epoch : 294, loss : 0.3205073771045305\n",
      "epoch : 295, loss : 0.32044740706003566\n",
      "epoch : 296, loss : 0.32038133451354134\n",
      "epoch : 297, loss : 0.32032261243407695\n",
      "epoch : 298, loss : 0.320261189797099\n",
      "epoch : 299, loss : 0.32020050952528684\n",
      "epoch : 300, loss : 0.32013959086738353\n",
      "epoch validation: 300, loss : 0.30461971814087196\n",
      "epoch : 301, loss : 0.32007879381205223\n",
      "epoch : 302, loss : 0.3200202551237199\n",
      "epoch : 303, loss : 0.3199604004007844\n",
      "epoch : 304, loss : 0.3199020309171589\n",
      "epoch : 305, loss : 0.3198436936544067\n",
      "epoch : 306, loss : 0.31978286874964257\n",
      "epoch : 307, loss : 0.31972756235708677\n",
      "epoch : 308, loss : 0.3196687865894028\n",
      "epoch : 309, loss : 0.31961151977365676\n",
      "epoch : 310, loss : 0.3195513717815742\n",
      "epoch validation: 310, loss : 0.3040930224337088\n",
      "epoch : 311, loss : 0.3194956833446153\n",
      "epoch : 312, loss : 0.31944086452830556\n",
      "epoch : 313, loss : 0.31937784633360977\n",
      "epoch : 314, loss : 0.3193247433094842\n",
      "epoch : 315, loss : 0.31926834995519054\n",
      "epoch : 316, loss : 0.3192123403346758\n",
      "epoch : 317, loss : 0.31915518831346157\n",
      "epoch : 318, loss : 0.3191002865733449\n",
      "epoch : 319, loss : 0.319044109650476\n",
      "epoch : 320, loss : 0.3189871122215896\n",
      "epoch validation: 320, loss : 0.3035748758057291\n",
      "epoch : 321, loss : 0.3189366251598437\n",
      "epoch : 322, loss : 0.3188795648341955\n",
      "epoch : 323, loss : 0.3188286611499886\n",
      "epoch : 324, loss : 0.3187748944069179\n",
      "epoch : 325, loss : 0.31871962428901546\n",
      "epoch : 326, loss : 0.31866336699806214\n",
      "epoch : 327, loss : 0.31861403517338427\n",
      "epoch : 328, loss : 0.3185555183314249\n",
      "epoch : 329, loss : 0.31850396303546635\n",
      "epoch : 330, loss : 0.3184504843887317\n",
      "epoch validation: 330, loss : 0.3030690330896498\n",
      "epoch : 331, loss : 0.318398186734827\n",
      "epoch : 332, loss : 0.3183477930074861\n",
      "epoch : 333, loss : 0.31829566466297177\n",
      "epoch : 334, loss : 0.3182443000679446\n",
      "epoch : 335, loss : 0.31819004466569467\n",
      "epoch : 336, loss : 0.31814235140201313\n",
      "epoch : 337, loss : 0.3180910402569602\n",
      "epoch : 338, loss : 0.31803822749864685\n",
      "epoch : 339, loss : 0.31798713649586996\n",
      "epoch : 340, loss : 0.31793268210463976\n",
      "epoch validation: 340, loss : 0.30259933204267375\n",
      "epoch : 341, loss : 0.3178803786678716\n",
      "epoch : 342, loss : 0.317831513641561\n",
      "epoch : 343, loss : 0.31778334952212234\n",
      "epoch : 344, loss : 0.3177312711694155\n",
      "epoch : 345, loss : 0.31768350546111085\n",
      "epoch : 346, loss : 0.31763258645700854\n",
      "epoch : 347, loss : 0.3175820951646035\n",
      "epoch : 348, loss : 0.31753329160558275\n",
      "epoch : 349, loss : 0.3174851793442066\n",
      "epoch : 350, loss : 0.31743446572921996\n",
      "epoch validation: 350, loss : 0.3021412291094776\n",
      "epoch : 351, loss : 0.31738551419098365\n",
      "epoch : 352, loss : 0.3173365447157633\n",
      "epoch : 353, loss : 0.3172874266151772\n",
      "epoch : 354, loss : 0.3172414053534699\n",
      "epoch : 355, loss : 0.3171932734332459\n",
      "epoch : 356, loss : 0.31714792833699623\n",
      "epoch : 357, loss : 0.31709485963463435\n",
      "epoch : 358, loss : 0.31704769743779715\n",
      "epoch : 359, loss : 0.31700077730541426\n",
      "epoch : 360, loss : 0.3169531397045005\n",
      "epoch validation: 360, loss : 0.3016829577585061\n",
      "epoch : 361, loss : 0.31690204172041414\n",
      "epoch : 362, loss : 0.31685660916647707\n",
      "epoch : 363, loss : 0.31680807286996193\n",
      "epoch : 364, loss : 0.31676165198243056\n",
      "epoch : 365, loss : 0.31671386337369795\n",
      "epoch : 366, loss : 0.31667001448326215\n",
      "epoch : 367, loss : 0.3166212094947696\n",
      "epoch : 368, loss : 0.31657484671563024\n",
      "epoch : 369, loss : 0.31652590255577895\n",
      "epoch : 370, loss : 0.31648321433151644\n",
      "epoch validation: 370, loss : 0.3012545523342005\n",
      "epoch : 371, loss : 0.3164360884526499\n",
      "epoch : 372, loss : 0.3163897500395082\n",
      "epoch : 373, loss : 0.31634450605312403\n",
      "epoch : 374, loss : 0.3162958575069702\n",
      "epoch : 375, loss : 0.31625001798853164\n",
      "epoch : 376, loss : 0.3162061528324388\n",
      "epoch : 377, loss : 0.316161591989212\n",
      "epoch : 378, loss : 0.31611971128298794\n",
      "epoch : 379, loss : 0.31607220840979683\n",
      "epoch : 380, loss : 0.31602950769931426\n",
      "epoch validation: 380, loss : 0.30081812318327816\n",
      "epoch : 381, loss : 0.31598068567267223\n",
      "epoch : 382, loss : 0.31593939241079627\n",
      "epoch : 383, loss : 0.31589328596917116\n",
      "epoch : 384, loss : 0.3158473506526545\n",
      "epoch : 385, loss : 0.315799998386597\n",
      "epoch : 386, loss : 0.31575985509151405\n",
      "epoch : 387, loss : 0.31571339527072834\n",
      "epoch : 388, loss : 0.3156732723781074\n",
      "epoch : 389, loss : 0.3156280062926659\n",
      "epoch : 390, loss : 0.31558444561046917\n",
      "epoch validation: 390, loss : 0.3004067349283732\n",
      "epoch : 391, loss : 0.3155399901077671\n",
      "epoch : 392, loss : 0.3154952917175815\n",
      "epoch : 393, loss : 0.31545283350833625\n",
      "epoch : 394, loss : 0.3154116011614543\n",
      "epoch : 395, loss : 0.31536279850126814\n",
      "epoch : 396, loss : 0.31532139144837856\n",
      "epoch : 397, loss : 0.3152799118570117\n",
      "epoch : 398, loss : 0.3152364508552087\n",
      "epoch : 399, loss : 0.31519391335720237\n",
      "epoch : 400, loss : 0.3151516342001368\n",
      "epoch validation: 400, loss : 0.299997240319386\n",
      "epoch : 401, loss : 0.31510609394127087\n",
      "epoch : 402, loss : 0.3150646645005417\n",
      "epoch : 403, loss : 0.31502251928787706\n",
      "epoch : 404, loss : 0.31498036253475403\n",
      "epoch : 405, loss : 0.3149382433341455\n",
      "epoch : 406, loss : 0.3148942211599544\n",
      "epoch : 407, loss : 0.31485521980712927\n",
      "epoch : 408, loss : 0.3148101683283701\n",
      "epoch : 409, loss : 0.31477011007783834\n",
      "epoch : 410, loss : 0.31473031398919665\n",
      "epoch validation: 410, loss : 0.29958979147233705\n",
      "epoch : 411, loss : 0.3146876972529662\n",
      "epoch : 412, loss : 0.3146447939004085\n",
      "epoch : 413, loss : 0.31460288591509644\n",
      "epoch : 414, loss : 0.31455951946780314\n",
      "epoch : 415, loss : 0.3145171468680392\n",
      "epoch : 416, loss : 0.3144785990259906\n",
      "epoch : 417, loss : 0.3144340729340911\n",
      "epoch : 418, loss : 0.31439120794263803\n",
      "epoch : 419, loss : 0.31435343730371706\n",
      "epoch : 420, loss : 0.31431375927826055\n",
      "epoch validation: 420, loss : 0.29919081000798897\n",
      "epoch : 421, loss : 0.3142730207687836\n",
      "epoch : 422, loss : 0.31422811878715146\n",
      "epoch : 423, loss : 0.3141896112157797\n",
      "epoch : 424, loss : 0.31414847286028225\n",
      "epoch : 425, loss : 0.31410692109141586\n",
      "epoch : 426, loss : 0.31406655171236325\n",
      "epoch : 427, loss : 0.3140257095571521\n",
      "epoch : 428, loss : 0.3139853232121456\n",
      "epoch : 429, loss : 0.3139450744393664\n",
      "epoch : 430, loss : 0.3139044888996223\n",
      "epoch validation: 430, loss : 0.29879138422335766\n",
      "epoch : 431, loss : 0.3138633148217675\n",
      "epoch : 432, loss : 0.3138242010099366\n",
      "epoch : 433, loss : 0.31378310235838097\n",
      "epoch : 434, loss : 0.3137433472984694\n",
      "epoch : 435, loss : 0.31370201016696847\n",
      "epoch : 436, loss : 0.3136619392082153\n",
      "epoch : 437, loss : 0.3136254648367564\n",
      "epoch : 438, loss : 0.3135854975038836\n",
      "epoch : 439, loss : 0.31354397641451554\n",
      "epoch : 440, loss : 0.31350679522336916\n",
      "epoch validation: 440, loss : 0.2984126952382945\n",
      "epoch : 441, loss : 0.3134638249852456\n",
      "epoch : 442, loss : 0.31342217429764857\n",
      "epoch : 443, loss : 0.3133859436186536\n",
      "epoch : 444, loss : 0.31334648138027665\n",
      "epoch : 445, loss : 0.3133033471824529\n",
      "epoch : 446, loss : 0.31326579020264766\n",
      "epoch : 447, loss : 0.31322674208515605\n",
      "epoch : 448, loss : 0.31318692019021555\n",
      "epoch : 449, loss : 0.31314734467341343\n",
      "epoch : 450, loss : 0.3131075657745318\n",
      "epoch validation: 450, loss : 0.2980382905969786\n",
      "epoch : 451, loss : 0.31307054915313803\n",
      "epoch : 452, loss : 0.31302825581325694\n",
      "epoch : 453, loss : 0.3129886756254028\n",
      "epoch : 454, loss : 0.3129506322533585\n",
      "epoch : 455, loss : 0.31291123000587256\n",
      "epoch : 456, loss : 0.3128730811994494\n",
      "epoch : 457, loss : 0.3128368288581801\n",
      "epoch : 458, loss : 0.31279614517580745\n",
      "epoch : 459, loss : 0.3127575604273945\n",
      "epoch : 460, loss : 0.3127205822465205\n",
      "epoch validation: 460, loss : 0.2976490670396376\n",
      "epoch : 461, loss : 0.31267906057236844\n",
      "epoch : 462, loss : 0.31264234421587034\n",
      "epoch : 463, loss : 0.31260238328313295\n",
      "epoch : 464, loss : 0.31256306542141377\n",
      "epoch : 465, loss : 0.31252547441584655\n",
      "epoch : 466, loss : 0.3124863963372024\n",
      "epoch : 467, loss : 0.31244790908887754\n",
      "epoch : 468, loss : 0.31241110134725425\n",
      "epoch : 469, loss : 0.3123693281762995\n",
      "epoch : 470, loss : 0.31233520001216336\n",
      "epoch validation: 470, loss : 0.2972769759191099\n",
      "epoch : 471, loss : 0.3122950690155921\n",
      "epoch : 472, loss : 0.31225715927750797\n",
      "epoch : 473, loss : 0.31221564092435117\n",
      "epoch : 474, loss : 0.3121796382822154\n",
      "epoch : 475, loss : 0.31214090052977667\n",
      "epoch : 476, loss : 0.3121041517322089\n",
      "epoch : 477, loss : 0.31206659795066644\n",
      "epoch : 478, loss : 0.3120289683060417\n",
      "epoch : 479, loss : 0.3119913597244683\n",
      "epoch : 480, loss : 0.3119530033811118\n",
      "epoch validation: 480, loss : 0.2969049173154572\n",
      "epoch : 481, loss : 0.3119168363677612\n",
      "epoch : 482, loss : 0.3118789772207076\n",
      "epoch : 483, loss : 0.3118398372271611\n",
      "epoch : 484, loss : 0.3118005556581441\n",
      "epoch : 485, loss : 0.3117631895705884\n",
      "epoch : 486, loss : 0.31172565234348526\n",
      "epoch : 487, loss : 0.31168972767976133\n",
      "epoch : 488, loss : 0.31165171887133586\n",
      "epoch : 489, loss : 0.3116160100975702\n",
      "epoch : 490, loss : 0.3115767092108293\n",
      "epoch validation: 490, loss : 0.296535644997922\n",
      "epoch : 491, loss : 0.3115375267681052\n",
      "epoch : 492, loss : 0.31149925100220033\n",
      "epoch : 493, loss : 0.3114628235831164\n",
      "epoch : 494, loss : 0.3114291929798706\n",
      "epoch : 495, loss : 0.3113883935608143\n",
      "epoch : 496, loss : 0.31135026095868196\n",
      "epoch : 497, loss : 0.31131517675221543\n",
      "epoch : 498, loss : 0.31127808262890044\n",
      "epoch : 499, loss : 0.3112381972776827\n",
      "epoch : 500, loss : 0.3112013367466744\n",
      "epoch validation: 500, loss : 0.29617787174187427\n",
      "epoch : 501, loss : 0.3111641855816566\n",
      "epoch : 502, loss : 0.3111293004002682\n",
      "epoch : 503, loss : 0.3110928050271243\n",
      "epoch : 504, loss : 0.31105712852115897\n",
      "epoch : 505, loss : 0.31101702335976467\n",
      "epoch : 506, loss : 0.31097797706521063\n",
      "epoch : 507, loss : 0.31094325739933654\n",
      "epoch : 508, loss : 0.31090653490621684\n",
      "epoch : 509, loss : 0.3108719944621819\n",
      "epoch : 510, loss : 0.31082995617653914\n",
      "epoch validation: 510, loss : 0.2957998345382223\n",
      "epoch : 511, loss : 0.31079446829046853\n",
      "epoch : 512, loss : 0.31075418723184006\n",
      "epoch : 513, loss : 0.3107183320455022\n",
      "epoch : 514, loss : 0.31068086577940357\n",
      "epoch : 515, loss : 0.3106465374206214\n",
      "epoch : 516, loss : 0.3106067657665631\n",
      "epoch : 517, loss : 0.31057297615623175\n",
      "epoch : 518, loss : 0.3105336376610248\n",
      "epoch : 519, loss : 0.3105011862745985\n",
      "epoch : 520, loss : 0.31046373770723856\n",
      "epoch validation: 520, loss : 0.29544699509707534\n",
      "epoch : 521, loss : 0.3104255372376809\n",
      "epoch : 522, loss : 0.3103897594558003\n",
      "epoch : 523, loss : 0.31035278070458144\n",
      "epoch : 524, loss : 0.3103142683755231\n",
      "epoch : 525, loss : 0.31027682207871315\n",
      "epoch : 526, loss : 0.31024317077700364\n",
      "epoch : 527, loss : 0.3102055509726322\n",
      "epoch : 528, loss : 0.3101699614009365\n",
      "epoch : 529, loss : 0.3101333669791099\n",
      "epoch : 530, loss : 0.3100941628795262\n",
      "epoch validation: 530, loss : 0.2950829270822826\n",
      "epoch : 531, loss : 0.31005827359884053\n",
      "epoch : 532, loss : 0.3100215481818011\n",
      "epoch : 533, loss : 0.30998482269256616\n",
      "epoch : 534, loss : 0.3099495817964911\n",
      "epoch : 535, loss : 0.30991360389216\n",
      "epoch : 536, loss : 0.30987347452359837\n",
      "epoch : 537, loss : 0.3098368022460924\n",
      "epoch : 538, loss : 0.3098018056380945\n",
      "epoch : 539, loss : 0.3097665241609017\n",
      "epoch : 540, loss : 0.30972927474857187\n",
      "epoch validation: 540, loss : 0.2947171439900417\n",
      "epoch : 541, loss : 0.3096929222576378\n",
      "epoch : 542, loss : 0.3096593042272468\n",
      "epoch : 543, loss : 0.3096216871013302\n",
      "epoch : 544, loss : 0.3095853016062012\n",
      "epoch : 545, loss : 0.3095482093816291\n",
      "epoch : 546, loss : 0.3095104988057946\n",
      "epoch : 547, loss : 0.3094750304071074\n",
      "epoch : 548, loss : 0.3094382437862918\n",
      "epoch : 549, loss : 0.30940028119745644\n",
      "epoch : 550, loss : 0.3093662437795149\n",
      "epoch validation: 550, loss : 0.29435253813285234\n",
      "epoch : 551, loss : 0.30932646965997856\n",
      "epoch : 552, loss : 0.30929205952890854\n",
      "epoch : 553, loss : 0.3092565833619803\n",
      "epoch : 554, loss : 0.3092189750836281\n",
      "epoch : 555, loss : 0.3091830383632477\n",
      "epoch : 556, loss : 0.30914770864137964\n",
      "epoch : 557, loss : 0.3091118848131782\n",
      "epoch : 558, loss : 0.3090765816737111\n",
      "epoch : 559, loss : 0.3090406829101402\n",
      "epoch : 560, loss : 0.30900220706510223\n",
      "epoch validation: 560, loss : 0.29398870642852876\n",
      "epoch : 561, loss : 0.30896795428783974\n",
      "epoch : 562, loss : 0.3089314656685893\n",
      "epoch : 563, loss : 0.3088939915728835\n",
      "epoch : 564, loss : 0.30885782858443467\n",
      "epoch : 565, loss : 0.3088237652916086\n",
      "epoch : 566, loss : 0.308785982173687\n",
      "epoch : 567, loss : 0.3087495626888938\n",
      "epoch : 568, loss : 0.308710197741238\n",
      "epoch : 569, loss : 0.30867665974322217\n",
      "epoch : 570, loss : 0.3086419614869379\n",
      "epoch validation: 570, loss : 0.29363248313871004\n",
      "epoch : 571, loss : 0.30860260447357285\n",
      "epoch : 572, loss : 0.30856904374117883\n",
      "epoch : 573, loss : 0.30853330147228847\n",
      "epoch : 574, loss : 0.308494733971392\n",
      "epoch : 575, loss : 0.308457936070022\n",
      "epoch : 576, loss : 0.3084228675552579\n",
      "epoch : 577, loss : 0.30838771086807054\n",
      "epoch : 578, loss : 0.30834834832994107\n",
      "epoch : 579, loss : 0.30831469433353276\n",
      "epoch : 580, loss : 0.30827882225704634\n",
      "epoch validation: 580, loss : 0.29327037646038123\n",
      "epoch : 581, loss : 0.30824212986225075\n",
      "epoch : 582, loss : 0.30820616773625675\n",
      "epoch : 583, loss : 0.3081687782140261\n",
      "epoch : 584, loss : 0.3081324461291638\n",
      "epoch : 585, loss : 0.3080958237099035\n",
      "epoch : 586, loss : 0.3080629318259483\n",
      "epoch : 587, loss : 0.3080253304711493\n",
      "epoch : 588, loss : 0.30798837668876083\n",
      "epoch : 589, loss : 0.30795298972304247\n",
      "epoch : 590, loss : 0.3079143928818751\n",
      "epoch validation: 590, loss : 0.29290416194198204\n",
      "epoch : 591, loss : 0.3078781187592089\n",
      "epoch : 592, loss : 0.30784207971607763\n",
      "epoch : 593, loss : 0.3078064686770356\n",
      "epoch : 594, loss : 0.3077694008148514\n",
      "epoch : 595, loss : 0.3077340544230312\n",
      "epoch : 596, loss : 0.30769462656417446\n",
      "epoch : 597, loss : 0.3076617020406118\n",
      "epoch : 598, loss : 0.3076241469090126\n",
      "epoch : 599, loss : 0.30758820439132956\n",
      "epoch : 600, loss : 0.30754914288878327\n",
      "epoch validation: 600, loss : 0.29254372983939886\n",
      "epoch : 601, loss : 0.30751342285973154\n",
      "epoch : 602, loss : 0.30747870017247375\n",
      "epoch : 603, loss : 0.3074409772927129\n",
      "epoch : 604, loss : 0.3074081774779357\n",
      "epoch : 605, loss : 0.3073677074943864\n",
      "epoch : 606, loss : 0.30733340013367955\n",
      "epoch : 607, loss : 0.3072973565133505\n",
      "epoch : 608, loss : 0.30726041955252487\n",
      "epoch : 609, loss : 0.3072239607969631\n",
      "epoch : 610, loss : 0.307188043807626\n",
      "epoch validation: 610, loss : 0.2921853232406831\n",
      "epoch : 611, loss : 0.3071500431753464\n",
      "epoch : 612, loss : 0.30711375388439544\n",
      "epoch : 613, loss : 0.307077465979599\n",
      "epoch : 614, loss : 0.30704189888237754\n",
      "epoch : 615, loss : 0.30700769729536864\n",
      "epoch : 616, loss : 0.30696844203801926\n",
      "epoch : 617, loss : 0.3069304522386817\n",
      "epoch : 618, loss : 0.3068971689682194\n",
      "epoch : 619, loss : 0.30685903722521407\n",
      "epoch : 620, loss : 0.3068235475360596\n",
      "epoch validation: 620, loss : 0.2918197436965713\n",
      "epoch : 621, loss : 0.30678450156999537\n",
      "epoch : 622, loss : 0.3067485369460354\n",
      "epoch : 623, loss : 0.30671113637788583\n",
      "epoch : 624, loss : 0.30667642527440375\n",
      "epoch : 625, loss : 0.30663798003666853\n",
      "epoch : 626, loss : 0.30660002794145613\n",
      "epoch : 627, loss : 0.30656462157679387\n",
      "epoch : 628, loss : 0.30652616214541384\n",
      "epoch : 629, loss : 0.30648973308749095\n",
      "epoch : 630, loss : 0.3064538641731695\n",
      "epoch validation: 630, loss : 0.2914480133660773\n",
      "epoch : 631, loss : 0.3064154827346404\n",
      "epoch : 632, loss : 0.306381040611918\n",
      "epoch : 633, loss : 0.30634391850380355\n",
      "epoch : 634, loss : 0.30630514905390926\n",
      "epoch : 635, loss : 0.3062692606792778\n",
      "epoch : 636, loss : 0.30623163811342663\n",
      "epoch : 637, loss : 0.30619555394807757\n",
      "epoch : 638, loss : 0.30615929675526743\n",
      "epoch : 639, loss : 0.30611832434484776\n",
      "epoch : 640, loss : 0.3060833854752398\n",
      "epoch validation: 640, loss : 0.29107597031334576\n",
      "epoch : 641, loss : 0.30605031818137035\n",
      "epoch : 642, loss : 0.30601098008323896\n",
      "epoch : 643, loss : 0.3059715957438588\n",
      "epoch : 644, loss : 0.30593597369499564\n",
      "epoch : 645, loss : 0.30590036560875156\n",
      "epoch : 646, loss : 0.3058657840240833\n",
      "epoch : 647, loss : 0.30582683423331886\n",
      "epoch : 648, loss : 0.3057868575944399\n",
      "epoch : 649, loss : 0.3057518568243687\n",
      "epoch : 650, loss : 0.30571590672976284\n",
      "epoch validation: 650, loss : 0.2907181852027889\n",
      "epoch : 651, loss : 0.3056757283043261\n",
      "epoch : 652, loss : 0.3056394187950118\n",
      "epoch : 653, loss : 0.30560642835691343\n",
      "epoch : 654, loss : 0.3055662291788662\n",
      "epoch : 655, loss : 0.3055261206026225\n",
      "epoch : 656, loss : 0.3054911144575927\n",
      "epoch : 657, loss : 0.3054516263659899\n",
      "epoch : 658, loss : 0.3054179331186668\n",
      "epoch : 659, loss : 0.3053835051372474\n",
      "epoch : 660, loss : 0.30534254588988113\n",
      "epoch validation: 660, loss : 0.290348783770735\n",
      "epoch : 661, loss : 0.30530787448131647\n",
      "epoch : 662, loss : 0.30526627647842086\n",
      "epoch : 663, loss : 0.30523068244865004\n",
      "epoch : 664, loss : 0.30519056413322687\n",
      "epoch : 665, loss : 0.3051571202673769\n",
      "epoch : 666, loss : 0.305119681121536\n",
      "epoch : 667, loss : 0.305080789708092\n",
      "epoch : 668, loss : 0.30504435673356056\n",
      "epoch : 669, loss : 0.30500552633257455\n",
      "epoch : 670, loss : 0.30497003795376926\n",
      "epoch validation: 670, loss : 0.2899670670054449\n",
      "epoch : 671, loss : 0.304930287965595\n",
      "epoch : 672, loss : 0.30489378666519656\n",
      "epoch : 673, loss : 0.3048549377604915\n",
      "epoch : 674, loss : 0.3048164381234105\n",
      "epoch : 675, loss : 0.30477980944118643\n",
      "epoch : 676, loss : 0.3047413912442939\n",
      "epoch : 677, loss : 0.3047053030107257\n",
      "epoch : 678, loss : 0.3046655084742883\n",
      "epoch : 679, loss : 0.3046263699029941\n",
      "epoch : 680, loss : 0.30458755991168956\n",
      "epoch validation: 680, loss : 0.28958933976616047\n",
      "epoch : 681, loss : 0.30454981806796305\n",
      "epoch : 682, loss : 0.304510620054464\n",
      "epoch : 683, loss : 0.3044738165074021\n",
      "epoch : 684, loss : 0.30443969974294305\n",
      "epoch : 685, loss : 0.3043980707320594\n",
      "epoch : 686, loss : 0.3043608119415278\n",
      "epoch : 687, loss : 0.30432166308958747\n",
      "epoch : 688, loss : 0.3042829889666953\n",
      "epoch : 689, loss : 0.3042436893090837\n",
      "epoch : 690, loss : 0.3042078391908733\n",
      "epoch validation: 690, loss : 0.2892112734821416\n",
      "epoch : 691, loss : 0.3041684625330995\n",
      "epoch : 692, loss : 0.30413107538610235\n",
      "epoch : 693, loss : 0.30409228910381597\n",
      "epoch : 694, loss : 0.30405403723102786\n",
      "epoch : 695, loss : 0.3040150251679931\n",
      "epoch : 696, loss : 0.3039786647716465\n",
      "epoch : 697, loss : 0.30393488323243784\n",
      "epoch : 698, loss : 0.30389771168300694\n",
      "epoch : 699, loss : 0.30385810275771474\n",
      "epoch : 700, loss : 0.30382235264116825\n",
      "epoch validation: 700, loss : 0.28882727044266326\n",
      "epoch : 701, loss : 0.30378291660616563\n",
      "epoch : 702, loss : 0.3037417336735268\n",
      "epoch : 703, loss : 0.3037042960221338\n",
      "epoch : 704, loss : 0.30366609311597637\n",
      "epoch : 705, loss : 0.30362700917324875\n",
      "epoch : 706, loss : 0.30358882735360615\n",
      "epoch : 707, loss : 0.3035531726290378\n",
      "epoch : 708, loss : 0.30351082279974984\n",
      "epoch : 709, loss : 0.30347354171884267\n",
      "epoch : 710, loss : 0.3034325519814914\n",
      "epoch validation: 710, loss : 0.2884471704678018\n",
      "epoch : 711, loss : 0.30339329157659944\n",
      "epoch : 712, loss : 0.303357425030292\n",
      "epoch : 713, loss : 0.303317570188913\n",
      "epoch : 714, loss : 0.30328047666843083\n",
      "epoch : 715, loss : 0.3032379868736332\n",
      "epoch : 716, loss : 0.3031990476811752\n",
      "epoch : 717, loss : 0.3031586289348066\n",
      "epoch : 718, loss : 0.3031208193897046\n",
      "epoch : 719, loss : 0.3030805534957511\n",
      "epoch : 720, loss : 0.30304165628500457\n",
      "epoch validation: 720, loss : 0.288061742865762\n",
      "epoch : 721, loss : 0.3030045647210218\n",
      "epoch : 722, loss : 0.30296088418338535\n",
      "epoch : 723, loss : 0.3029246486997766\n",
      "epoch : 724, loss : 0.3028895783476358\n",
      "epoch : 725, loss : 0.30284498810999155\n",
      "epoch : 726, loss : 0.3028077247206035\n",
      "epoch : 727, loss : 0.3027670698947916\n",
      "epoch : 728, loss : 0.3027276783160028\n",
      "epoch : 729, loss : 0.3026894705962072\n",
      "epoch : 730, loss : 0.3026491013326675\n",
      "epoch validation: 730, loss : 0.2876813942349927\n",
      "epoch : 731, loss : 0.30260777430854335\n",
      "epoch : 732, loss : 0.3025687793702927\n",
      "epoch : 733, loss : 0.3025301485611198\n",
      "epoch : 734, loss : 0.30248988252999476\n",
      "epoch : 735, loss : 0.30245070920376355\n",
      "epoch : 736, loss : 0.30241098710472963\n",
      "epoch : 737, loss : 0.30236920227058406\n",
      "epoch : 738, loss : 0.30232861595112803\n",
      "epoch : 739, loss : 0.3022886025611051\n",
      "epoch : 740, loss : 0.3022483733497098\n",
      "epoch validation: 740, loss : 0.2872880027420068\n",
      "epoch : 741, loss : 0.3022099853919227\n",
      "epoch : 742, loss : 0.30216894317562726\n",
      "epoch : 743, loss : 0.30213155296392913\n",
      "epoch : 744, loss : 0.3020896425853346\n",
      "epoch : 745, loss : 0.3020509506806789\n",
      "epoch : 746, loss : 0.30201160012159584\n",
      "epoch : 747, loss : 0.30196883583932305\n",
      "epoch : 748, loss : 0.30193159935360614\n",
      "epoch : 749, loss : 0.3018903170329656\n",
      "epoch : 750, loss : 0.30185050109415784\n",
      "epoch validation: 750, loss : 0.2868909262582775\n",
      "epoch : 751, loss : 0.30180803645228926\n",
      "epoch : 752, loss : 0.3017699248879397\n",
      "epoch : 753, loss : 0.30173152924493646\n",
      "epoch : 754, loss : 0.3016849129800071\n",
      "epoch : 755, loss : 0.3016474854793082\n",
      "epoch : 756, loss : 0.30160588044817593\n",
      "epoch : 757, loss : 0.30156530763289724\n",
      "epoch : 758, loss : 0.3015246324298918\n",
      "epoch : 759, loss : 0.3014853838578502\n",
      "epoch : 760, loss : 0.30144284344133887\n",
      "epoch validation: 760, loss : 0.28647845290427987\n",
      "epoch : 761, loss : 0.30140397696604215\n",
      "epoch : 762, loss : 0.30136011835808557\n",
      "epoch : 763, loss : 0.3013239575788205\n",
      "epoch : 764, loss : 0.301281009252046\n",
      "epoch : 765, loss : 0.30124158741790774\n",
      "epoch : 766, loss : 0.3011999599700577\n",
      "epoch : 767, loss : 0.30115723754676393\n",
      "epoch : 768, loss : 0.3011163905761096\n",
      "epoch : 769, loss : 0.30107376706793676\n",
      "epoch : 770, loss : 0.3010355435824383\n",
      "epoch validation: 770, loss : 0.28607685410583666\n",
      "epoch : 771, loss : 0.30099292980983505\n",
      "epoch : 772, loss : 0.30095197789392675\n",
      "epoch : 773, loss : 0.30091036161292317\n",
      "epoch : 774, loss : 0.30086839595419723\n",
      "epoch : 775, loss : 0.3008289901294218\n",
      "epoch : 776, loss : 0.30079158802711686\n",
      "epoch : 777, loss : 0.30075106570179494\n",
      "epoch : 778, loss : 0.3007059838485637\n",
      "epoch : 779, loss : 0.30066515275371053\n",
      "epoch : 780, loss : 0.30062175935220925\n",
      "epoch validation: 780, loss : 0.2856979658899381\n",
      "epoch : 781, loss : 0.3005819669238819\n",
      "epoch : 782, loss : 0.3005424446188087\n",
      "epoch : 783, loss : 0.3005003245229753\n",
      "epoch : 784, loss : 0.3004601113823091\n",
      "epoch : 785, loss : 0.30041703099182754\n",
      "epoch : 786, loss : 0.3003777642373371\n",
      "epoch : 787, loss : 0.3003346648864997\n",
      "epoch : 788, loss : 0.3002948928046827\n",
      "epoch : 789, loss : 0.3002556105606373\n",
      "epoch : 790, loss : 0.3002098451816123\n",
      "epoch validation: 790, loss : 0.285280000866037\n",
      "epoch : 791, loss : 0.300170800821899\n",
      "epoch : 792, loss : 0.3001311287760388\n",
      "epoch : 793, loss : 0.30008652262649566\n",
      "epoch : 794, loss : 0.3000473871776589\n",
      "epoch : 795, loss : 0.3000078699551523\n",
      "epoch : 796, loss : 0.29996765773497924\n",
      "epoch : 797, loss : 0.2999241087195832\n",
      "epoch : 798, loss : 0.29988487535914365\n",
      "epoch : 799, loss : 0.2998426747688836\n",
      "epoch : 800, loss : 0.2998003580845719\n",
      "epoch validation: 800, loss : 0.28488883332804193\n",
      "epoch : 801, loss : 0.2997596240385847\n",
      "epoch : 802, loss : 0.29971548287891026\n",
      "epoch : 803, loss : 0.29967520695699507\n",
      "epoch : 804, loss : 0.2996312303887319\n",
      "epoch : 805, loss : 0.29958980520393036\n",
      "epoch : 806, loss : 0.29954941139931884\n",
      "epoch : 807, loss : 0.2995074151062168\n",
      "epoch : 808, loss : 0.2994662471974716\n",
      "epoch : 809, loss : 0.2994244074802701\n",
      "epoch : 810, loss : 0.29938448102451687\n",
      "epoch validation: 810, loss : 0.2844716858032138\n",
      "epoch : 811, loss : 0.29933785557992354\n",
      "epoch : 812, loss : 0.29929944486609955\n",
      "epoch : 813, loss : 0.2992541058559346\n",
      "epoch : 814, loss : 0.2992158821249713\n",
      "epoch : 815, loss : 0.29917090271722324\n",
      "epoch : 816, loss : 0.29913075383420484\n",
      "epoch : 817, loss : 0.2990866982551335\n",
      "epoch : 818, loss : 0.2990474529210623\n",
      "epoch : 819, loss : 0.299003035176632\n",
      "epoch : 820, loss : 0.2989598270469569\n",
      "epoch validation: 820, loss : 0.2840649760364331\n",
      "epoch : 821, loss : 0.298919982421248\n",
      "epoch : 822, loss : 0.2988758129789674\n",
      "epoch : 823, loss : 0.298832967909876\n",
      "epoch : 824, loss : 0.29879358692758884\n",
      "epoch : 825, loss : 0.2987498305990657\n",
      "epoch : 826, loss : 0.29870541005344814\n",
      "epoch : 827, loss : 0.2986646458983075\n",
      "epoch : 828, loss : 0.29862205701150984\n",
      "epoch : 829, loss : 0.29857625507626884\n",
      "epoch : 830, loss : 0.2985375584752927\n",
      "epoch validation: 830, loss : 0.2836592096200054\n",
      "epoch : 831, loss : 0.29849229081959805\n",
      "epoch : 832, loss : 0.29845056494394706\n",
      "epoch : 833, loss : 0.29840624492341117\n",
      "epoch : 834, loss : 0.29836416631835955\n",
      "epoch : 835, loss : 0.2983182649048724\n",
      "epoch : 836, loss : 0.29827453958794703\n",
      "epoch : 837, loss : 0.29823374562326443\n",
      "epoch : 838, loss : 0.29818881096953687\n",
      "epoch : 839, loss : 0.29814359599493384\n",
      "epoch : 840, loss : 0.29810236088657216\n",
      "epoch validation: 840, loss : 0.28325788471703384\n",
      "epoch : 841, loss : 0.29806081905181325\n",
      "epoch : 842, loss : 0.2980143991948733\n",
      "epoch : 843, loss : 0.2979728159451381\n",
      "epoch : 844, loss : 0.29792641869623415\n",
      "epoch : 845, loss : 0.29788576585940957\n",
      "epoch : 846, loss : 0.2978395553623406\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expérimentations\n",
    "## Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# deux couches caché de 100 neurones pour avoir le suraprentissage\n",
    "# l2 marche pas de ouf par contre dropout efficace\n",
    "# Modules (torch, nn, F et optim)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.functional import one_hot\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Progrès\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#matpotlib \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "\n",
    "\n",
    "TB_PATH = \"/tmp/logs/deep_tp3\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "mean=[0.5]\n",
    "std=[0.5]\n",
    "batchsize=128\n",
    "\n",
    "#Transformations à appliquer sur le dataset (transformation des images en tenseurs et normalization pour obtenir des valeurs entre -1 et 1)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "# Téléchargement des données (via le dataset specifique MNIST de pytorch)\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "print(len(trainset))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, pin_memory=True, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation d'une image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img):\n",
    "  if img.dim()==2 or ((img.dim()==3) and (img.size()[0]==1)):\n",
    "      return img*std[0]+mean[0]\n",
    "  return img * img.new(std).view(3, 1, 1) + img.new(mean).view(3, 1, 1)\n",
    "\n",
    "# Recuperation du premier batch\n",
    "imgs,labs=next(iter(trainloader))\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = imgs.shape[2],imgs.shape[3] # taille de l'image\n",
    "\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#Visualisation de la première image\n",
    "print(imgs.size())\n",
    "img = unnormalize(imgs[0]) # pour retrouver l'image d'origine (avant normalisation)\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img.squeeze(),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "print(next(iter(train_loader)))\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$\\texttt{Softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^d x_j}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y} $$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire un réseau générique NN qui étend nn.Module, dont on la construction se fait par:\n",
    "# LinearMultiClass(inSize, outSize, layers=[], finalActivation=None, activation=nn.Tanh), où:\n",
    "# inSize est la dimension des entrées, outSize la dimension des sorties\n",
    "# liste des tailles des éventuelles couches cachées\n",
    "# finalActivation est la fonction d'activation a appliquer sur la sortie (si pas None)\n",
    "# et activation la fonction d'activation à appliquer après chaque couche cachée\n",
    "# \n",
    "\n",
    "# Comparer différentes architectures sur les dataset défini ci-dessus, en reportant l'accuracy en train et test pour chacune dans tensorboard (en plus de la loss considérée)\n",
    "# Pour ce dataset on doit définir 10 sorties pour chaque réseau, une par classe. \n",
    "# Comparer l'utilisation de la fonction d'activation tanh et ReLU\n",
    "\n",
    "\n",
    "# Comparer les deux types de coûts décrits si dessus (MLE avec un vecteur one hot cible et CrossEntropie qui paraît plus adapté pour ce cas) \n",
    "# Pour la cross entropie, on utilise nn.CrossEntropyLoss, qui n'a pas besoin que l'on ne rescale les sorties (la cross entropy combine un softmax + NLLloss)\n",
    "# Si on utilisait une BCE loss (qui n'est pas approprié ici, vu que les classes sont dépendantes : une seule classe est visée par exemple),\n",
    "# dans ce cas il faudrait ajouter une sigmoide en derniere couche.\n",
    "\n",
    "## [[student]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [[/student]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n",
    "\n",
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser.\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Vous pouvez aussi observer les histogrammes de la distribution des poids des différentes couches en utilisant la fonction addWeightsHisto ci dessous.  Utilisez pour ces experiences un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requiert que les modules soient enregistrés dans une liste model.layers\n",
    "def addWeightsHisto(writer,model,epoch):                \n",
    "    ix = 0\n",
    "    for module in model.layers:\n",
    "        if isinstance(module, nn.Linear):\n",
    "           writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "           ix += 1\n",
    "\n",
    "            \n",
    "## [[student]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [[/student]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [[student]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [[/student]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [[student]]\n",
    "\n",
    "\n",
    "\n",
    "## [[/student]]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
