{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Deep Learning  \n",
    "\n",
    "\n",
    "## TP3 : Méthodologie, Expérimentations et Régularisation \n",
    "\n",
    "Sylvain Lamprier (sylvain.lamprier@univ-angers.fr)\n",
    "\n",
    "Supports adaptés de Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) et Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:46:29.291517Z",
     "start_time": "2024-11-29T10:46:29.278162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.5.1+cu124\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from networkx import config\n",
    "from torch.onnx.symbolic_opset9 import batch_norm\n",
    "\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "## Chargement des données California_Housing et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "data_x = torch.tensor(housing['data'],dtype=torch.float)\n",
    "data_y = torch.tensor(housing['target'],dtype=torch.float).view(-1)\n",
    "Xdim = data_x.size(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGIDLqItECDu"
   },
   "source": [
    "# Méthodologie expérimentale et boîte à outils\n",
    "Pytorch dispose d'un ensemble d'outils qui permettent de simplifier les démarches expérimentales. Nous allons voir en particulier : \n",
    "* le DataLoader qui permet de gérer le chargement de données, le partitionement et la constitution d'ensembles de test et d'apprentissage; \n",
    "* le checkpointing qui permet de sauvegarder/charger les modèles en cours d'entraînement.\n",
    "* le TensorBoard (qui vient de tensorflow) qui permet de suivre l'évolution en apprentissage de vos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ4MoJP4k4i0"
   },
   "source": [
    "\n",
    "## DataLoader\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*. \n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous verrons une customization de cette fonction dans une séance ultérieure). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:46:29.320599Z",
     "start_time": "2024-11-29T10:46:29.313326Z"
    },
    "id": "AZaWAFO8k8ze"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 (tensor([   4.0368,   52.0000,    4.7617,    1.1036,  413.0000,    2.1399,\n",
      "          37.8500, -122.2500]), tensor(2.6970))\n",
      "1290 [tensor([[ 1.7813e+00,  2.9000e+01,  4.6486e+00,  9.1990e-01,  1.2780e+03,\n",
      "          3.3023e+00,  3.6430e+01, -1.1969e+02],\n",
      "        [ 6.2712e+00,  4.7000e+01,  6.9491e+00,  1.0291e+00,  6.9700e+02,\n",
      "          2.5345e+00,  3.7940e+01, -1.2232e+02],\n",
      "        [ 4.2062e+00,  3.5000e+01,  4.7975e+00,  9.3865e-01,  4.9900e+02,\n",
      "          3.0613e+00,  3.4090e+01, -1.1793e+02],\n",
      "        [ 4.1250e+00,  1.9000e+01,  5.3737e+00,  1.0303e+00,  1.3170e+03,\n",
      "          2.2172e+00,  3.7940e+01, -1.2202e+02],\n",
      "        [ 7.7382e+00,  3.8000e+01,  5.4823e+00,  1.1062e+00,  1.2510e+03,\n",
      "          2.1096e+00,  3.4070e+01, -1.1843e+02],\n",
      "        [ 3.7125e+00,  3.1000e+01,  5.3363e+00,  9.9246e-01,  1.7150e+03,\n",
      "          2.5867e+00,  3.4160e+01, -1.1726e+02],\n",
      "        [ 3.9699e+00,  1.4000e+01,  5.3754e+00,  1.0336e+00,  6.3010e+03,\n",
      "          2.2273e+00,  3.4230e+01, -1.1896e+02],\n",
      "        [ 2.9583e+00,  2.3000e+01,  7.1220e+00,  1.3659e+00,  9.2000e+01,\n",
      "          2.2439e+00,  3.8900e+01, -1.2260e+02],\n",
      "        [ 4.0690e+00,  2.8000e+01,  5.2473e+00,  9.7173e-01,  8.2000e+02,\n",
      "          2.8975e+00,  3.2690e+01, -1.1707e+02],\n",
      "        [ 3.5750e+00,  5.1000e+01,  6.0604e+00,  1.0805e+00,  7.0900e+02,\n",
      "          2.3792e+00,  3.3960e+01, -1.1740e+02],\n",
      "        [ 5.8632e+00,  5.2000e+01,  6.9562e+00,  1.0741e+00,  9.8100e+02,\n",
      "          3.3030e+00,  3.4070e+01, -1.1834e+02],\n",
      "        [ 4.3125e+00,  3.7000e+01,  6.9453e+00,  1.1572e+00,  1.2870e+03,\n",
      "          2.9317e+00,  3.8500e+01, -1.2248e+02],\n",
      "        [ 3.6917e+00,  3.5000e+01,  6.0478e+00,  1.1116e+00,  8.5700e+02,\n",
      "          3.4143e+00,  3.3910e+01, -1.1812e+02],\n",
      "        [ 1.7945e+00,  3.7000e+01,  4.5026e+00,  1.1563e+00,  1.9410e+03,\n",
      "          2.0367e+00,  3.4010e+01, -1.1834e+02],\n",
      "        [ 3.1215e+00,  3.6000e+01,  3.8563e+00,  1.0460e+00,  1.6590e+03,\n",
      "          3.1782e+00,  3.3860e+01, -1.1819e+02],\n",
      "        [ 3.5417e+00,  1.8000e+01,  3.4004e+00,  1.0057e+00,  9.5000e+02,\n",
      "          1.8199e+00,  3.4160e+01, -1.1826e+02]]), tensor([0.5790, 2.6770, 1.6130, 2.6710, 5.0000, 0.9800, 2.1700, 0.9170, 1.5330,\n",
      "        1.2550, 4.5000, 2.7650, 1.9750, 1.0630, 1.5360, 1.7710])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(data_x,data_y)\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True ) \n",
    "\n",
    "#Premier batch (aléatoire) du dataloader :\n",
    "print(len(iter(loader)),next(iter(loader)))\n",
    "\n",
    "## Exemple d'un Dataset (sans utilité dans le cas présent, TensorDataset permet de faire la même chose)\n",
    "class MyDataSet(Dataset):\n",
    "  def __init__(self, x,y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "  def __getitem__(self,i):\n",
    "    return self.x[i],self.y[i]\n",
    "  def __len__(self):\n",
    "    return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-29T10:46:29.370190Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, loss : 8.44908553759257\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "EPS=1e-4\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "# La boucle d'apprentissage :\n",
    "for i in range(EPOCHS):\n",
    "    cumloss = 0\n",
    "    # On parcourt tous les exemples par batch de 16 (paramètre batch_size de DataLoader)\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if i % 10==0: print(f\"iteration : {i}, loss : {cumloss/len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9x2LC_6lCQm"
   },
   "source": [
    "## Checkpointing\n",
    "Les modèles Deep sont généralement long à apprendre. Afin de ne pas perdre des résultats en cours de calcul, il est fortement recommander de faire du **checkpointing**, c'est-à-dire d'enregistrer des points d'étapes du modèle en cours d'apprentissage pour pouvoir reprendre à n'importe quel moment l'apprentissage du modèle en cas de problème.  Il s'agit en pratique de sauvegarder l'état du modèle et de l'optimisateur (et de tout autre objet qui peut servir lors de l'apprentissage) toutes les n itérations. Toutes les variables d'intérêt sont en général disponibles par la méthode **state_dict()** des modèles et de l'optimiseur. \n",
    "\n",
    "En pratique, vous pouvez utilisé un code dérivé de celui ci-dessous.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:16:07.540848Z",
     "start_time": "2024-11-29T10:16:07.535840Z"
    },
    "id": "URQTq8hrPJO0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def save_state(epoch,model,optim,fichier):\n",
    "      \"\"\" sauvegarde du modèle et de l'état de l'optimiseur dans fichier \"\"\"\n",
    "      state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "      torch.save(state,fichier)\n",
    " \n",
    "def load_state(fichier,model,optim):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle et l'optimiseur \"\"\"\n",
    "      epoch = 0\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          optim.load_state_dict(state['optim_state'])\n",
    "          epoch = state['epoch']\n",
    "      return epoch\n",
    "\n",
    "fichier = \"/tmp/netSeq.pth\" \n",
    "save_state(EPOCHS,netSeq,optim,fichier)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:16:46.364924Z",
     "start_time": "2024-11-29T10:16:07.584397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42655/475055523.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(fichier)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100, loss : 1.3314216262379357\n",
      "epoch : 110, loss : 1.3314291658789612\n",
      "epoch : 120, loss : 1.3313745029674944\n",
      "epoch : 130, loss : 1.3313496572102688\n",
      "epoch : 140, loss : 1.3312658595946407\n",
      "epoch : 150, loss : 1.3311576210944227\n",
      "epoch : 160, loss : 1.3311525393133015\n",
      "epoch : 170, loss : 1.3311549580836481\n",
      "epoch : 180, loss : 1.331074503364489\n",
      "epoch : 190, loss : 1.330887097443721\n"
     ]
    }
   ],
   "source": [
    "# On crée un autre réseau similaire à celui sauvegardé et on charge les poids \n",
    "# on peut observer qu'on repart du même point que le réseau précédent\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "start_epoch = load_state(fichier,netSeq,optim)\n",
    "for epoch in range(start_epoch,start_epoch+EPOCHS):\n",
    "    cumloss = 0\n",
    "    for bx,by in loader:\n",
    "        loss = mseloss(netSeq(bx).view(-1),by)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        cumloss += loss.item()\n",
    "    if epoch % 10 ==0: \n",
    "        save_state(epoch,netSeq,optim,fichier)\n",
    "        print(f\"epoch : {epoch}, loss : {cumloss/len(loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IstQCvKblSvT"
   },
   "source": [
    "\n",
    "## GPU \n",
    "Afin d'utiliser un GPU lors des calculs, il est nécessaire de transférer les données et le modèle sur le GPU par l'intermédiaire de la fonction **to(device)** des tenseurs et des modules.  Il est impossible de faire une opération lorsqu'une partie des tenseurs sont sur GPU et l'autre sur CPU. Il faut que tous les tenseurs et paramètres soient sur le même device ! On doit donc s'assurer que le modèle, les exemples et les labels sont sur GPU pour faire les opérations.\n",
    "\n",
    "Par ailleurs, on peut connaître le device sur lequel est chargé un tenseur par l'intermédiaire de ```.device``` (mais pas pour un modèle, il faut aller voir les paramètres dans ce cas).\n",
    "\n",
    "Une manière simple d'utiliser un GPU quand il existe et donc d'avoir un code agnostique est la suivante : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:16:46.778382Z",
     "start_time": "2024-11-29T10:16:46.376344Z"
    },
    "id": "Fs8s7EwwlWTn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device du mini-batch :  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "loader = DataLoader(TensorDataset(data_x,data_y), batch_size=16,shuffle=True ) \n",
    "\n",
    "## On charge le modèle sur GPU\n",
    "## A faire avant la déclaration de l'optimiseur, sinon les paramètres optimisés ne seront pas les mêmes! \n",
    "\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netSeq = netSeq.to(device)\n",
    "optim = torch.optim.Adam(params=netSeq.parameters(),lr=EPS)\n",
    "\n",
    "for i,(bx,by) in enumerate(loader):\n",
    "    ## On charge le batch sur GPU\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    loss = mseloss(netSeq(bx).view(-1),by)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "print(\"Device du mini-batch : \", bx.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5J1b55_lFR-"
   },
   "source": [
    "\n",
    "## TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.tensorboard``` qui permet de faire le pont de pytorch vers cet outil. \n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:03.109208Z",
     "start_time": "2024-11-29T10:16:46.795763Z"
    },
    "id": "1kIhHDnElQd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 5.799375577860101\n",
      "epoch : 1, loss : 5.487418466667796\n",
      "epoch : 2, loss : 5.1886165247407074\n",
      "epoch : 3, loss : 4.902273186694744\n",
      "epoch : 4, loss : 4.628070184981176\n",
      "epoch : 5, loss : 4.365507363348968\n",
      "epoch : 6, loss : 4.114989611298539\n",
      "epoch : 7, loss : 3.87585249390713\n",
      "epoch : 8, loss : 3.6482387152291085\n",
      "epoch : 9, loss : 3.4318542650503705\n",
      "epoch : 10, loss : 3.2264794360759645\n",
      "epoch : 11, loss : 3.033013763192088\n",
      "epoch : 12, loss : 2.850596846397533\n",
      "epoch : 13, loss : 2.679334432094596\n",
      "epoch : 14, loss : 2.519628061666045\n",
      "epoch : 15, loss : 2.370890045420144\n",
      "epoch : 16, loss : 2.233030836596045\n",
      "epoch : 17, loss : 2.1061494263567666\n",
      "epoch : 18, loss : 1.9907217646638553\n",
      "epoch : 19, loss : 1.8857101055425267\n",
      "epoch : 20, loss : 1.7909961455783179\n",
      "epoch : 21, loss : 1.7067061338775842\n",
      "epoch : 22, loss : 1.6328243989699571\n",
      "epoch : 23, loss : 1.5691422272675721\n",
      "epoch : 24, loss : 1.515131795822188\n",
      "epoch : 25, loss : 1.4698175875137942\n",
      "epoch : 26, loss : 1.4331821091877397\n",
      "epoch : 27, loss : 1.4043943944943043\n",
      "epoch : 28, loss : 1.382508460010669\n",
      "epoch : 29, loss : 1.366496532462364\n",
      "epoch : 30, loss : 1.355435010236363\n",
      "epoch : 31, loss : 1.3479767774195635\n",
      "epoch : 32, loss : 1.3430634699126547\n",
      "epoch : 33, loss : 1.3400049942870473\n",
      "epoch : 34, loss : 1.3381471774836844\n",
      "epoch : 35, loss : 1.3369841099709503\n",
      "epoch : 36, loss : 1.336266106921573\n",
      "epoch : 37, loss : 1.3358411445174105\n",
      "epoch : 38, loss : 1.3355858823356703\n",
      "epoch : 39, loss : 1.3354193055121473\n",
      "epoch : 40, loss : 1.3353146102770355\n",
      "epoch : 41, loss : 1.335238207379977\n",
      "epoch : 42, loss : 1.3351958093486092\n",
      "epoch : 43, loss : 1.3351558199224547\n",
      "epoch : 44, loss : 1.3351325747232103\n",
      "epoch : 45, loss : 1.3351168307454087\n",
      "epoch : 46, loss : 1.33509458675403\n",
      "epoch : 47, loss : 1.3350851392330125\n",
      "epoch : 48, loss : 1.335069053117619\n",
      "epoch : 49, loss : 1.3350547839966855\n",
      "epoch : 50, loss : 1.3350435482439145\n",
      "epoch : 51, loss : 1.3350310258163038\n",
      "epoch : 52, loss : 1.3350213598373324\n",
      "epoch : 53, loss : 1.33501246851082\n",
      "epoch : 54, loss : 1.3349991459948147\n",
      "epoch : 55, loss : 1.334988993013552\n",
      "epoch : 56, loss : 1.3349824695162071\n",
      "epoch : 57, loss : 1.3349685287406279\n",
      "epoch : 58, loss : 1.3349618404641632\n",
      "epoch : 59, loss : 1.3349477201931237\n",
      "epoch : 60, loss : 1.3349401851495106\n",
      "epoch : 61, loss : 1.3349282502897026\n",
      "epoch : 62, loss : 1.3349204525698064\n",
      "epoch : 63, loss : 1.3349153980150703\n",
      "epoch : 64, loss : 1.3349003544611524\n",
      "epoch : 65, loss : 1.3348856671142948\n",
      "epoch : 66, loss : 1.334878586544547\n",
      "epoch : 67, loss : 1.33487269973108\n",
      "epoch : 68, loss : 1.3348636038543642\n",
      "epoch : 69, loss : 1.3348521478416384\n",
      "epoch : 70, loss : 1.3348423633695572\n",
      "epoch : 71, loss : 1.3348279010648876\n",
      "epoch : 72, loss : 1.3348204408620679\n",
      "epoch : 73, loss : 1.3348078671351884\n",
      "epoch : 74, loss : 1.3347940565310707\n",
      "epoch : 75, loss : 1.3347833650056706\n",
      "epoch : 76, loss : 1.3347690831783205\n",
      "epoch : 77, loss : 1.3347586911778118\n",
      "epoch : 78, loss : 1.3347391843333725\n",
      "epoch : 79, loss : 1.3347220761831418\n",
      "epoch : 80, loss : 1.3347096382878547\n",
      "epoch : 81, loss : 1.3346905009922132\n",
      "epoch : 82, loss : 1.33466497244992\n",
      "epoch : 83, loss : 1.334639386913573\n",
      "epoch : 84, loss : 1.3346144475216088\n",
      "epoch : 85, loss : 1.3345806592425635\n",
      "epoch : 86, loss : 1.3345471773960793\n",
      "epoch : 87, loss : 1.334507317875707\n",
      "epoch : 88, loss : 1.3344634322926054\n",
      "epoch : 89, loss : 1.334422623002252\n",
      "epoch : 90, loss : 1.3343855030314866\n",
      "epoch : 91, loss : 1.3343347350640815\n",
      "epoch : 92, loss : 1.3342884982972183\n",
      "epoch : 93, loss : 1.3342430008012196\n",
      "epoch : 94, loss : 1.3341983323757962\n",
      "epoch : 95, loss : 1.3341561534030493\n",
      "epoch : 96, loss : 1.3341168649205866\n",
      "epoch : 97, loss : 1.3340728807818982\n",
      "epoch : 98, loss : 1.3340411654507467\n",
      "epoch : 99, loss : 1.334006157956382\n",
      "epoch : 0, loss : 3.381581679379293\n",
      "epoch : 1, loss : 3.1757511118123696\n",
      "epoch : 2, loss : 2.9839368505995405\n",
      "epoch : 3, loss : 2.804592787064323\n",
      "epoch : 4, loss : 2.6366635159697642\n",
      "epoch : 5, loss : 2.479632152466811\n",
      "epoch : 6, loss : 2.3345121104818904\n",
      "epoch : 7, loss : 2.2003662173369136\n",
      "epoch : 8, loss : 2.076745177990244\n",
      "epoch : 9, loss : 1.9642233294456504\n",
      "epoch : 10, loss : 1.8624037146106247\n",
      "epoch : 11, loss : 1.7714665816504827\n",
      "epoch : 12, loss : 1.6906705199748047\n",
      "epoch : 13, loss : 1.6196915830983671\n",
      "epoch : 14, loss : 1.5585935339678165\n",
      "epoch : 15, loss : 1.5070288189621859\n",
      "epoch : 16, loss : 1.4647482688343803\n",
      "epoch : 17, loss : 1.430772199637668\n",
      "epoch : 18, loss : 1.4042807966239692\n",
      "epoch : 19, loss : 1.3842845711366152\n",
      "epoch : 20, loss : 1.3699138690334882\n",
      "epoch : 21, loss : 1.3598869092242662\n",
      "epoch : 22, loss : 1.3533241087152053\n",
      "epoch : 23, loss : 1.3490668139947477\n",
      "epoch : 24, loss : 1.3463219950596492\n",
      "epoch : 25, loss : 1.3445948758097581\n",
      "epoch : 26, loss : 1.3435253169994021\n",
      "epoch : 27, loss : 1.342849995091904\n",
      "epoch : 28, loss : 1.3424016176268112\n",
      "epoch : 29, loss : 1.3421069597551065\n",
      "epoch : 30, loss : 1.3419005471375562\n",
      "epoch : 31, loss : 1.341757674827132\n",
      "epoch : 32, loss : 1.341630960550419\n",
      "epoch : 33, loss : 1.341528833871202\n",
      "epoch : 34, loss : 1.3414280028075212\n",
      "epoch : 35, loss : 1.3413320315438648\n",
      "epoch : 36, loss : 1.3412283770104712\n",
      "epoch : 37, loss : 1.3411198922599008\n",
      "epoch : 38, loss : 1.3409895172876904\n",
      "epoch : 39, loss : 1.3408589210159094\n",
      "epoch : 40, loss : 1.3407490316287491\n",
      "epoch : 41, loss : 1.3406381487384322\n",
      "epoch : 42, loss : 1.3405230770739474\n",
      "epoch : 43, loss : 1.3404240833696468\n",
      "epoch : 44, loss : 1.3403194106479948\n",
      "epoch : 45, loss : 1.3402232649252397\n",
      "epoch : 46, loss : 1.34012349190176\n",
      "epoch : 47, loss : 1.340015746133272\n",
      "epoch : 48, loss : 1.3399043705689815\n",
      "epoch : 49, loss : 1.3397755640421727\n",
      "epoch : 50, loss : 1.3396226008040035\n",
      "epoch : 51, loss : 1.3394586962553883\n",
      "epoch : 52, loss : 1.3392899091391601\n",
      "epoch : 53, loss : 1.3391305414512176\n",
      "epoch : 54, loss : 1.3389826365566069\n",
      "epoch : 55, loss : 1.3388459783191828\n",
      "epoch : 56, loss : 1.3387243883785351\n",
      "epoch : 57, loss : 1.3386017121778901\n",
      "epoch : 58, loss : 1.338485466775506\n",
      "epoch : 59, loss : 1.3383742053379384\n",
      "epoch : 60, loss : 1.3382541210729946\n",
      "epoch : 61, loss : 1.3381391634774762\n",
      "epoch : 62, loss : 1.3380281924277313\n",
      "epoch : 63, loss : 1.3379175297280614\n",
      "epoch : 64, loss : 1.3377947480410568\n",
      "epoch : 65, loss : 1.3376581268486127\n",
      "epoch : 66, loss : 1.337520917972853\n",
      "epoch : 67, loss : 1.3373823149952777\n",
      "epoch : 68, loss : 1.3372134920700576\n",
      "epoch : 69, loss : 1.3370399671469548\n",
      "epoch : 70, loss : 1.3368298419801763\n",
      "epoch : 71, loss : 1.3366068452365638\n",
      "epoch : 72, loss : 1.3363534020130023\n",
      "epoch : 73, loss : 1.336090927137885\n",
      "epoch : 74, loss : 1.3358401167069294\n",
      "epoch : 75, loss : 1.3355849524570065\n",
      "epoch : 76, loss : 1.3353301515643912\n",
      "epoch : 77, loss : 1.3351040130437806\n",
      "epoch : 78, loss : 1.3348941796971845\n",
      "epoch : 79, loss : 1.3346757511297862\n",
      "epoch : 80, loss : 1.3344807911519856\n",
      "epoch : 81, loss : 1.3343021863190703\n",
      "epoch : 82, loss : 1.3341460553019546\n",
      "epoch : 83, loss : 1.3340101164440776\n",
      "epoch : 84, loss : 1.3338959018851435\n",
      "epoch : 85, loss : 1.3338068058324415\n",
      "epoch : 86, loss : 1.3337141175371732\n",
      "epoch : 87, loss : 1.333636988006359\n",
      "epoch : 88, loss : 1.333570404847463\n",
      "epoch : 89, loss : 1.3335108557412791\n",
      "epoch : 90, loss : 1.3334604152182277\n",
      "epoch : 91, loss : 1.3334107114363087\n",
      "epoch : 92, loss : 1.3333729094544122\n",
      "epoch : 93, loss : 1.3333411842815637\n",
      "epoch : 94, loss : 1.333314196892487\n",
      "epoch : 95, loss : 1.3332862187956656\n",
      "epoch : 96, loss : 1.3332642635171728\n",
      "epoch : 97, loss : 1.3332432803488516\n",
      "epoch : 98, loss : 1.3332290456507556\n",
      "epoch : 99, loss : 1.3332166297491206\n"
     ]
    }
   ],
   "source": [
    "# Pour observer les courbes produites, il faut lancer tensorboard \n",
    "# à la main à partir du shell :  tensorboard --logdir /tmp/logs/deep\n",
    "TB_PATH = \"/tmp/logs/deep\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class DeuxCouches(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeuxCouches,self).__init__()\n",
    "    self.un = torch.nn.Linear(Xdim,5)\n",
    "    self.act = torch.nn.Tanh()\n",
    "    self.deux = torch.nn.Linear(5,1)\n",
    "  def forward(self,x):\n",
    "    return self.deux(self.act(self.un(x)))\n",
    "\n",
    "EPS = 1e-5\n",
    "EPOCHS=100\n",
    "netSeq = torch.nn.Sequential(torch.nn.Linear(Xdim,5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "netDeuxCouches = DeuxCouches()\n",
    "netSeq.name = \"Sequentiel\"\n",
    "netDeuxCouches.name = \"DeuxCouches\"\n",
    "## Obtention d'un SummaryWriter \n",
    "summary = SummaryWriter(f\"{TB_PATH}/test\")\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for model in [netSeq, netDeuxCouches]:\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=EPS) \n",
    "    for i in range(EPOCHS):\n",
    "        cumloss = 0\n",
    "        for boston_x, boston_y in loader:\n",
    "            loss = mseloss(model(boston_x),boston_y.view(-1,1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()  \n",
    "            cumloss+= loss.item()\n",
    "        summary.add_scalar(f\"loss/{model.name}\",cumloss/len(loader),i)\n",
    "        print(f\"epoch : {i}, loss : {cumloss/len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des courbes dans tensorboard\n",
    "![Capture du tensorboard](./img/tensorBoard1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaW5Av4elaBN"
   },
   "source": [
    "## Dernières remarques et exemple typique de code\n",
    "* Le graphe de calcul est instancié de manière dynamique sous pytorch, et cela consomme des ressources. Lorsqu'il n'y a pas de rétropropagation qui intervient - lors de l'évaluation d'un modèle par exemple -, il faut à tout prix éviter de le calculer. L'environnement **torch.no_grad()** permet de désactiver temporairement l'instanciation du graphe. **Toutes les procédures d'évaluation doivent se faire dans cet environnement afin d'économiser du temps !**\n",
    "* Pour certains modules, le comportement est différent entre l'évaluation et l'apprentissage (pour le dropout ou la batchnormalisation par exemple, ou pour les RNNs). Afin d'indiquer à pytorch dans quelle phase on se situe, deux méthodes sont disponibles dans la classe module,  **.train()** et **.eval()** qui permettent de basculer entre les deux environnements.\n",
    "\n",
    "Les deux fonctionalités sont très différentes : **no_grad** agit au niveau du graphe de calcul et désactive sa construction (comme si les variables avaient leur propriété **requires_grad** à False), alors que **eval/train** agissent au niveau du module et influence le comportement du module.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple typique de code pytorch qui reprend l'ensemble des éléments de ce tutoriel. Vous êtes prêt maintenant à expérimenter la puissance de ce framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:03.125683Z",
     "start_time": "2024-11-29T10:18:03.122332Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "TB_PATH = \"/tmp/logs/module1\"\n",
    "MODEL_PATH = \"/tmp/models\"\n",
    "os.makedirs(MODEL_PATH,exist_ok=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:03.166924Z",
     "start_time": "2024-11-29T10:18:03.159732Z"
    },
    "id": "b3TRg2p5ldCJ"
   },
   "outputs": [],
   "source": [
    "def save_state(fichier,epoch,model,optim):\n",
    "    state = {'epoch' : epoch, 'model_state': model.state_dict(), 'optim_state': optim.state_dict()}\n",
    "    torch.save(state,fichier)\n",
    "\n",
    "def load_state(fichier,model,optim):\n",
    "    epoch = 0\n",
    "    if os.path.isfile(fichier):\n",
    "        state = torch.load(fichier)\n",
    "        model.load_state_dict(state['model_state'])\n",
    "        optim.load_state_dict(state['optim_state'])\n",
    "        epoch = state['epoch']\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train(model, loss, epochs, train_loader, test_loader,lr=1e-3):\n",
    "    # On créé un writer avec la date du modèle pour s'y retrouver\n",
    "    check_file = f\"{MODEL_PATH}/{model.name}.pth\"\n",
    "    summary = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(params=model.parameters(),lr=lr)\n",
    "    start_epoch = load_state(check_file,model,optim)\n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        # Apprentissage\n",
    "        # .train() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "        model.train()\n",
    "        cumloss = 0\n",
    "        for xbatch, ybatch in train_loader:\n",
    "            xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "            outputs = model(xbatch)\n",
    "            l = loss(outputs.view(-1),ybatch)\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l.item()\n",
    "        summary.add_scalar(\"loss/train\",  cumloss/len(train_loader),epoch)\n",
    "        print(f\"epoch : {epoch}, loss : {cumloss/len(train_loader)}\")\n",
    "        \n",
    "        if epoch % 10 == 0: \n",
    "            save_state(check_file,epoch,model,optim)\n",
    "            # Validation\n",
    "            # .eval() inutile tant qu'on utilise pas de normalisation ou de récurrent\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss = 0\n",
    "                for xbatch, ybatch in test_loader:\n",
    "                    xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
    "                    outputs = model(xbatch)\n",
    "                    cumloss += loss(outputs.view(-1),ybatch).item()\n",
    "            summary.add_scalar(\"loss/validation\", cumloss/len(test_loader) ,epoch)\n",
    "            print(f\"epoch validation: {epoch}, loss : {cumloss/len(test_loader)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:34.776908Z",
     "start_time": "2024-11-29T10:18:03.215586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 1.2535664746321218\n",
      "epoch validation: 0, loss : 1.236368711027064\n",
      "epoch : 1, loss : 1.22589987990006\n",
      "epoch : 2, loss : 1.1996882099282833\n",
      "epoch : 3, loss : 1.1744747397800286\n",
      "epoch : 4, loss : 1.1498940571449523\n",
      "epoch : 5, loss : 1.1259144559906897\n",
      "epoch : 6, loss : 1.1025981295189655\n",
      "epoch : 7, loss : 1.0800588135571443\n",
      "epoch : 8, loss : 1.0581529892172463\n",
      "epoch : 9, loss : 1.036910327194735\n",
      "epoch : 10, loss : 1.016422776086617\n",
      "epoch validation: 10, loss : 0.9993881801533144\n",
      "epoch : 11, loss : 0.9966348245913206\n",
      "epoch : 12, loss : 0.9773591482477595\n",
      "epoch : 13, loss : 0.9587463360539702\n",
      "epoch : 14, loss : 0.9406913151285907\n",
      "epoch : 15, loss : 0.923165406425332\n",
      "epoch : 16, loss : 0.9061997810941796\n",
      "epoch : 17, loss : 0.8897338026282631\n",
      "epoch : 18, loss : 0.8737144703262074\n",
      "epoch : 19, loss : 0.8581577244019786\n",
      "epoch : 20, loss : 0.842925636776427\n",
      "epoch validation: 20, loss : 0.8279010540870733\n",
      "epoch : 21, loss : 0.8280285736329334\n",
      "epoch : 22, loss : 0.8134475855893174\n",
      "epoch : 23, loss : 0.7992016802606887\n",
      "epoch : 24, loss : 0.7852189027263906\n",
      "epoch : 25, loss : 0.7714833389216846\n",
      "epoch : 26, loss : 0.7580077040420715\n",
      "epoch : 27, loss : 0.7447674897636554\n",
      "epoch : 28, loss : 0.7317396721463333\n",
      "epoch : 29, loss : 0.7188805056981338\n",
      "epoch : 30, loss : 0.7062310201455225\n",
      "epoch validation: 30, loss : 0.6963490232132202\n",
      "epoch : 31, loss : 0.6937903748002163\n",
      "epoch : 32, loss : 0.6815189420021781\n",
      "epoch : 33, loss : 0.6695730350767226\n",
      "epoch : 34, loss : 0.6578870444998953\n",
      "epoch : 35, loss : 0.6464225177241619\n",
      "epoch : 36, loss : 0.635278471074132\n",
      "epoch : 37, loss : 0.624470120202028\n",
      "epoch : 38, loss : 0.61387061195578\n",
      "epoch : 39, loss : 0.6036674904225524\n",
      "epoch : 40, loss : 0.5938627947295128\n",
      "epoch validation: 40, loss : 0.5917193351616693\n",
      "epoch : 41, loss : 0.5844359875678323\n",
      "epoch : 42, loss : 0.575368783559448\n",
      "epoch : 43, loss : 0.5666878754778426\n",
      "epoch : 44, loss : 0.5584642000120971\n",
      "epoch : 45, loss : 0.5506427806997022\n",
      "epoch : 46, loss : 0.5432299041701841\n",
      "epoch : 47, loss : 0.536226978222298\n",
      "epoch : 48, loss : 0.5296158614792219\n",
      "epoch : 49, loss : 0.523434554396356\n",
      "epoch : 50, loss : 0.5176147136606218\n",
      "epoch validation: 50, loss : 0.5232696084435596\n",
      "epoch : 51, loss : 0.5121016984285657\n",
      "epoch : 52, loss : 0.5069369079126406\n",
      "epoch : 53, loss : 0.5021110441203612\n",
      "epoch : 54, loss : 0.4975691955002357\n",
      "epoch : 55, loss : 0.49327938724309206\n",
      "epoch : 56, loss : 0.4892509258965882\n",
      "epoch : 57, loss : 0.48547685597553963\n",
      "epoch : 58, loss : 0.481913303833657\n",
      "epoch : 59, loss : 0.4785124706609767\n",
      "epoch : 60, loss : 0.4752672305908199\n",
      "epoch validation: 60, loss : 0.4858008153563322\n",
      "epoch : 61, loss : 0.47216879837792514\n",
      "epoch : 62, loss : 0.4691996145704689\n",
      "epoch : 63, loss : 0.4663255540654063\n",
      "epoch : 64, loss : 0.4635263816200832\n",
      "epoch : 65, loss : 0.4608390656581452\n",
      "epoch : 66, loss : 0.458220632920085\n",
      "epoch : 67, loss : 0.45565018511211225\n",
      "epoch : 68, loss : 0.45313489501357307\n",
      "epoch : 69, loss : 0.4506518762962994\n",
      "epoch : 70, loss : 0.4482217822512684\n",
      "epoch validation: 70, loss : 0.4606167867259924\n",
      "epoch : 71, loss : 0.44581255103757444\n",
      "epoch : 72, loss : 0.4434140905333582\n",
      "epoch : 73, loss : 0.4410134372265302\n",
      "epoch : 74, loss : 0.4386450276933899\n",
      "epoch : 75, loss : 0.4362921281970402\n",
      "epoch : 76, loss : 0.4339544317908874\n",
      "epoch : 77, loss : 0.43164801527565533\n",
      "epoch : 78, loss : 0.42933894178375254\n",
      "epoch : 79, loss : 0.42704490887708674\n",
      "epoch : 80, loss : 0.42477319457445495\n",
      "epoch validation: 80, loss : 0.4377439501153868\n",
      "epoch : 81, loss : 0.4225122412240725\n",
      "epoch : 82, loss : 0.42028167756823026\n",
      "epoch : 83, loss : 0.41806618213769076\n",
      "epoch : 84, loss : 0.41586456335214683\n",
      "epoch : 85, loss : 0.413702338378039\n",
      "epoch : 86, loss : 0.4115628644520807\n",
      "epoch : 87, loss : 0.4094124847748714\n",
      "epoch : 88, loss : 0.4072984688908093\n",
      "epoch : 89, loss : 0.4051976994459712\n",
      "epoch : 90, loss : 0.4031163321865737\n",
      "epoch validation: 90, loss : 0.4161921563421109\n",
      "epoch : 91, loss : 0.40104857861793547\n",
      "epoch : 92, loss : 0.3989988150970303\n",
      "epoch : 93, loss : 0.3969723204504554\n",
      "epoch : 94, loss : 0.3949743915825736\n",
      "epoch : 95, loss : 0.3930095786069946\n",
      "epoch : 96, loss : 0.39107083186305075\n",
      "epoch : 97, loss : 0.3891615812024174\n",
      "epoch : 98, loss : 0.38727746767446747\n",
      "epoch : 99, loss : 0.3854206244338506\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "all_data = torch.tensor(housing['data'],dtype=torch.float)\n",
    "all_labels = torch.tensor(housing['target'],dtype=torch.float).view(-1)\n",
    "\n",
    "# Il est toujours bon de normaliser\n",
    "all_data = (all_data-all_data.mean(0))/all_data.std(0)\n",
    "all_labels = (all_labels-all_labels.mean())/all_labels.std()\n",
    "\n",
    "train_tensor_data = TensorDataset(all_data, all_labels)\n",
    "\n",
    "# Split en 80% apprentissage et 20% test\n",
    "train_size = int(0.8 * len(train_tensor_data))\n",
    "validate_size = len(train_tensor_data) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(train_tensor_data, [train_size, validate_size])\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.Linear(all_data.size(1),5),torch.nn.Tanh(),torch.nn.Linear(5,1))\n",
    "net.name = \"mon_premier_reseau_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "net = net.to(device)\n",
    "train(net,torch.nn.MSELoss(),EPOCHS,train_loader,valid_loader,lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expérimentations\n",
    "## Jeu de données MNIST\n",
    "Ce jeu de données est l'équivalent du *Hello world* en programmation. Chaque donnée est un chiffre manuscrit (de 0 à 9). Les lignes suivantes vous permettent de charger le jeu de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:35.134339Z",
     "start_time": "2024-11-29T10:18:34.786122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42655/219069740.py:20: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# deux couches caché de 100 neurones pour avoir le suraprentissage\n",
    "# l2 marche pas de ouf par contre dropout efficace\n",
    "# Modules (torch, nn, F et optim)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.functional import one_hot\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Progrès\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#matpotlib \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "\n",
    "\n",
    "TB_PATH = \"/tmp/logs/deep_tp3\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "mean=[0.5]\n",
    "std=[0.5]\n",
    "batchsize=128\n",
    "\n",
    "#Transformations à appliquer sur le dataset (transformation des images en tenseurs et normalization pour obtenir des valeurs entre -1 et 1)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "# Téléchargement des données (via le dataset specifique MNIST de pytorch)\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "print(len(trainset))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, pin_memory=True, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation d'une image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:35.260373Z",
     "start_time": "2024-11-29T10:18:35.155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7a83fb7b90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unnormalize(img):\n",
    "  if img.dim()==2 or ((img.dim()==3) and (img.size()[0]==1)):\n",
    "      return img*std[0]+mean[0]\n",
    "  return img * img.new(std).view(3, 1, 1) + img.new(mean).view(3, 1, 1)\n",
    "\n",
    "# Recuperation du premier batch\n",
    "imgs,labs=next(iter(trainloader))\n",
    "# dimension of images (flattened)\n",
    "HEIGHT,WIDTH = imgs.shape[2],imgs.shape[3] # taille de l'image\n",
    "\n",
    "INPUT_DIM = HEIGHT * WIDTH\n",
    "\n",
    "#Visualisation de la première image\n",
    "print(imgs.size())\n",
    "img = unnormalize(imgs[0]) # pour retrouver l'image d'origine (avant normalisation)\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img.squeeze(),cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:18:35.334606Z",
     "start_time": "2024-11-29T10:18:35.287946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]]]), tensor([8, 4, 8, 6, 4, 6, 1, 7, 3, 4, 2, 5, 6, 6, 7, 9, 6, 2, 1, 4, 9, 8, 1, 8,\n",
      "        4, 7, 6, 7, 4, 4, 1, 7, 4, 3, 0, 8, 3, 8, 6, 1, 6, 4, 7, 7, 8, 5, 6, 2,\n",
      "        2, 5, 1, 6, 1, 4, 4, 5, 2, 0, 3, 9, 9, 1, 4, 5, 8, 2, 1, 7, 2, 3, 9, 3,\n",
      "        2, 6, 1, 2, 1, 4, 0, 3, 4, 1, 3, 5, 7, 1, 6, 9, 6, 7, 6, 7, 2, 9, 6, 3,\n",
      "        5, 5, 7, 4, 7, 3, 1, 0, 4, 1, 1, 1, 9, 6, 4, 8, 5, 3, 2, 9, 1, 9, 9, 6,\n",
      "        7, 6, 2, 6, 2, 8, 5, 1, 1, 3, 7, 0, 0, 1, 8, 4, 2, 6, 1, 2, 0, 1, 3, 8,\n",
      "        8, 9, 3, 4, 0, 5, 8, 2, 2, 0, 9, 2, 6, 3, 0, 2, 6, 1, 4, 3, 8, 7, 1, 8,\n",
      "        1, 1, 1, 7, 3, 6, 1, 6, 7, 2, 0, 0, 2, 1, 0, 8, 0, 7, 0, 5, 2, 8, 9, 5,\n",
      "        8, 1, 3, 5, 0, 1, 0, 4, 0, 9, 4, 1, 1, 0, 8, 1, 1, 1, 1, 2, 7, 6, 2, 0,\n",
      "        0, 7, 0, 9, 5, 8, 8, 9, 9, 9, 8, 2, 0, 1, 9, 8, 4, 8, 0, 0, 3, 5, 4, 1,\n",
      "        3, 7, 5, 6, 3, 2, 6, 2, 9, 0, 4, 7, 1, 9, 4, 7, 4, 9, 1, 1, 3, 1, 0, 0,\n",
      "        0, 0, 8, 8, 2, 0, 5, 4, 1, 6, 6, 2, 1, 1, 5, 0, 2, 5, 5, 3, 5, 0, 1, 9,\n",
      "        5, 1, 5, 4, 9, 9, 8, 9, 2, 6, 8, 4])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## On utilise qu'une partie du training test pour mettre en évidence le sur-apprentissage\n",
    "TRAIN_RATIO = 0.01\n",
    "train_length = int(len(trainset)*TRAIN_RATIO)\n",
    "\n",
    "ds_train, ds_test =  torch.utils.data.random_split(trainset, (train_length, len(trainset)- train_length))\n",
    "\n",
    "#On utilise un DataLoader pour faciliter les manipulations, on fixe  la taille du mini batch à 300\n",
    "train_loader = DataLoader(ds_train,batch_size=300,shuffle=True)\n",
    "test_loader = DataLoader(ds_test,batch_size=300,shuffle=False)\n",
    "\n",
    "print(next(iter(train_loader)))\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Classification multi-labels, nombre de couches, fonction de coût </span>\n",
    "\n",
    "L'objectif est de classer chaque image parmi les 10 chiffres qu'ils représentent. Le réseau aura donc 10 sorties, une par classe, chacune représentant la probabilité d'appartenance à chaque classe. Pour garantir une distribution de probabilité en sortie, il faut utiliser le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html> **Softmax** </a> : $$\\texttt{Softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^d x_j}$$ qui permet de normaliser le vecteur de sortie.\n",
    "\n",
    "* Faites quelques exemples de réseau à 1, 2, 3 couches et en faisant varier les nombre de neurones par couche. Utilisez un coût moindre carré dans un premier temps. Pour superviser ce coût, on doit construire le vecteur one-hot correspondant à la classe : un vecteur qui ne contient que des 0 sauf à l'index de la classe qui contient un 1 (utilisez ```torch.nn.functional.one_hot```).  Comparez les courbes de coût et d'erreurs en apprentissage et en test selon l'architecture.\n",
    "* Le coût privilégié en multi-classe est la *cross-entropy**. Ce coût représente la négative log-vraisemblance : $$NNL(y,\\mathbf{x}) = -x_{y} $$ en notant $y$ l'indice de la classe et $\\mathbf{x}$ le vecteur de log-probabilité inféré. On peut utiliser soit son implémentation par le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>**NLLLoss**</a>, soit - plus pratique - le module <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>**CrossEntropyLoss** <a>  qui combine un *logSoftmax* et la cross entropie, ce qui évite d'avoir à ajouter un module de *Softmax* en sortie du réseau. Utilisez ce dernier coût et observez les changements.\n",
    "* Changez la fonction d'activation en une ReLU et observez l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T10:42:13.907028Z",
     "start_time": "2024-11-29T10:23:28.695091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training configuration: Tanh_MSE\n",
      "Epoch 1/2000 -> Train Loss: 0.0899, Train Acc: 0.1717, Test Loss: 0.0889, Test Acc: 0.1126\n",
      "Epoch 50/2000 -> Train Loss: 0.0012, Train Acc: 0.9967, Test Loss: 0.0204, Test Acc: 0.8594\n",
      "Epoch 100/2000 -> Train Loss: 0.0003, Train Acc: 0.9983, Test Loss: 0.0204, Test Acc: 0.8601\n",
      "Epoch 150/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.0204, Test Acc: 0.8607\n",
      "Epoch 200/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0206, Test Acc: 0.8596\n",
      "Epoch 250/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0208, Test Acc: 0.8596\n",
      "Epoch 300/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0209, Test Acc: 0.8595\n",
      "Epoch 350/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0209, Test Acc: 0.8593\n",
      "Epoch 400/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0210, Test Acc: 0.8593\n",
      "Epoch 450/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0211, Test Acc: 0.8592\n",
      "Epoch 500/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0211, Test Acc: 0.8591\n",
      "Epoch 550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0212, Test Acc: 0.8590\n",
      "Epoch 600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0212, Test Acc: 0.8589\n",
      "Epoch 650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0213, Test Acc: 0.8588\n",
      "Epoch 700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0213, Test Acc: 0.8588\n",
      "Epoch 750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0214, Test Acc: 0.8588\n",
      "Epoch 800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0214, Test Acc: 0.8587\n",
      "Epoch 850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0214, Test Acc: 0.8587\n",
      "Epoch 900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0215, Test Acc: 0.8585\n",
      "Epoch 950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0215, Test Acc: 0.8584\n",
      "Epoch 1000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0215, Test Acc: 0.8584\n",
      "Epoch 1050/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0216, Test Acc: 0.8585\n",
      "Epoch 1100/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0216, Test Acc: 0.8584\n",
      "Epoch 1150/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0216, Test Acc: 0.8584\n",
      "Epoch 1200/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0216, Test Acc: 0.8584\n",
      "Epoch 1250/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0217, Test Acc: 0.8584\n",
      "Epoch 1300/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0217, Test Acc: 0.8583\n",
      "Epoch 1350/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0217, Test Acc: 0.8583\n",
      "Epoch 1400/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0217, Test Acc: 0.8584\n",
      "Epoch 1450/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0218, Test Acc: 0.8583\n",
      "Epoch 1500/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0218, Test Acc: 0.8584\n",
      "Epoch 1550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0218, Test Acc: 0.8584\n",
      "Epoch 1600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0218, Test Acc: 0.8584\n",
      "Epoch 1650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0218, Test Acc: 0.8584\n",
      "Epoch 1700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0219, Test Acc: 0.8584\n",
      "Epoch 1750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0219, Test Acc: 0.8584\n",
      "Epoch 1800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0219, Test Acc: 0.8584\n",
      "Epoch 1850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0219, Test Acc: 0.8584\n",
      "Epoch 1900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0219, Test Acc: 0.8584\n",
      "Epoch 1950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0220, Test Acc: 0.8583\n",
      "Epoch 2000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0220, Test Acc: 0.8584\n",
      "\n",
      "Training configuration: Tanh_CrossEntropy\n",
      "Epoch 1/2000 -> Train Loss: 2.2999, Train Acc: 0.1350, Test Loss: 2.2437, Test Acc: 0.1139\n",
      "Epoch 50/2000 -> Train Loss: 0.0494, Train Acc: 1.0000, Test Loss: 0.4781, Test Acc: 0.8542\n",
      "Epoch 100/2000 -> Train Loss: 0.0100, Train Acc: 1.0000, Test Loss: 0.5095, Test Acc: 0.8554\n",
      "Epoch 150/2000 -> Train Loss: 0.0046, Train Acc: 1.0000, Test Loss: 0.5355, Test Acc: 0.8557\n",
      "Epoch 200/2000 -> Train Loss: 0.0027, Train Acc: 1.0000, Test Loss: 0.5564, Test Acc: 0.8555\n",
      "Epoch 250/2000 -> Train Loss: 0.0018, Train Acc: 1.0000, Test Loss: 0.5731, Test Acc: 0.8554\n",
      "Epoch 300/2000 -> Train Loss: 0.0013, Train Acc: 1.0000, Test Loss: 0.5873, Test Acc: 0.8554\n",
      "Epoch 350/2000 -> Train Loss: 0.0010, Train Acc: 1.0000, Test Loss: 0.5995, Test Acc: 0.8556\n",
      "Epoch 400/2000 -> Train Loss: 0.0007, Train Acc: 1.0000, Test Loss: 0.6106, Test Acc: 0.8555\n",
      "Epoch 450/2000 -> Train Loss: 0.0006, Train Acc: 1.0000, Test Loss: 0.6208, Test Acc: 0.8554\n",
      "Epoch 500/2000 -> Train Loss: 0.0005, Train Acc: 1.0000, Test Loss: 0.6293, Test Acc: 0.8555\n",
      "Epoch 550/2000 -> Train Loss: 0.0004, Train Acc: 1.0000, Test Loss: 0.6377, Test Acc: 0.8555\n",
      "Epoch 600/2000 -> Train Loss: 0.0004, Train Acc: 1.0000, Test Loss: 0.6452, Test Acc: 0.8554\n",
      "Epoch 650/2000 -> Train Loss: 0.0003, Train Acc: 1.0000, Test Loss: 0.6524, Test Acc: 0.8555\n",
      "Epoch 700/2000 -> Train Loss: 0.0003, Train Acc: 1.0000, Test Loss: 0.6588, Test Acc: 0.8556\n",
      "Epoch 750/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.6653, Test Acc: 0.8554\n",
      "Epoch 800/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.6713, Test Acc: 0.8554\n",
      "Epoch 850/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.6770, Test Acc: 0.8554\n",
      "Epoch 900/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.6825, Test Acc: 0.8552\n",
      "Epoch 950/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.6878, Test Acc: 0.8551\n",
      "Epoch 1000/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.6928, Test Acc: 0.8552\n",
      "Epoch 1050/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.6975, Test Acc: 0.8552\n",
      "Epoch 1100/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7025, Test Acc: 0.8552\n",
      "Epoch 1150/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7070, Test Acc: 0.8553\n",
      "Epoch 1200/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7115, Test Acc: 0.8553\n",
      "Epoch 1250/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7158, Test Acc: 0.8553\n",
      "Epoch 1300/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7201, Test Acc: 0.8553\n",
      "Epoch 1350/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7242, Test Acc: 0.8553\n",
      "Epoch 1400/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7282, Test Acc: 0.8553\n",
      "Epoch 1450/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7323, Test Acc: 0.8552\n",
      "Epoch 1500/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.7361, Test Acc: 0.8554\n",
      "Epoch 1550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7399, Test Acc: 0.8553\n",
      "Epoch 1600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7437, Test Acc: 0.8553\n",
      "Epoch 1650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7475, Test Acc: 0.8553\n",
      "Epoch 1700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7511, Test Acc: 0.8553\n",
      "Epoch 1750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7546, Test Acc: 0.8554\n",
      "Epoch 1800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7582, Test Acc: 0.8553\n",
      "Epoch 1850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7617, Test Acc: 0.8553\n",
      "Epoch 1900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7651, Test Acc: 0.8553\n",
      "Epoch 1950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7685, Test Acc: 0.8552\n",
      "Epoch 2000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.7719, Test Acc: 0.8553\n",
      "\n",
      "Training configuration: ReLU_CrossEntropy\n",
      "Epoch 1/2000 -> Train Loss: 2.3013, Train Acc: 0.1367, Test Loss: 2.2361, Test Acc: 0.3005\n",
      "Epoch 50/2000 -> Train Loss: 0.0625, Train Acc: 0.9983, Test Loss: 0.5015, Test Acc: 0.8537\n",
      "Epoch 100/2000 -> Train Loss: 0.0106, Train Acc: 1.0000, Test Loss: 0.5943, Test Acc: 0.8549\n",
      "Epoch 150/2000 -> Train Loss: 0.0040, Train Acc: 1.0000, Test Loss: 0.6716, Test Acc: 0.8535\n",
      "Epoch 200/2000 -> Train Loss: 0.0021, Train Acc: 1.0000, Test Loss: 0.7266, Test Acc: 0.8533\n",
      "Epoch 250/2000 -> Train Loss: 0.0013, Train Acc: 1.0000, Test Loss: 0.7710, Test Acc: 0.8525\n",
      "Epoch 300/2000 -> Train Loss: 0.0009, Train Acc: 1.0000, Test Loss: 0.8067, Test Acc: 0.8523\n",
      "Epoch 350/2000 -> Train Loss: 0.0006, Train Acc: 1.0000, Test Loss: 0.8387, Test Acc: 0.8520\n",
      "Epoch 400/2000 -> Train Loss: 0.0005, Train Acc: 1.0000, Test Loss: 0.8662, Test Acc: 0.8518\n",
      "Epoch 450/2000 -> Train Loss: 0.0004, Train Acc: 1.0000, Test Loss: 0.8904, Test Acc: 0.8514\n",
      "Epoch 500/2000 -> Train Loss: 0.0003, Train Acc: 1.0000, Test Loss: 0.9124, Test Acc: 0.8511\n",
      "Epoch 550/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.9323, Test Acc: 0.8508\n",
      "Epoch 600/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.9516, Test Acc: 0.8507\n",
      "Epoch 650/2000 -> Train Loss: 0.0002, Train Acc: 1.0000, Test Loss: 0.9692, Test Acc: 0.8505\n",
      "Epoch 700/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.9856, Test Acc: 0.8504\n",
      "Epoch 750/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0009, Test Acc: 0.8502\n",
      "Epoch 800/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0155, Test Acc: 0.8501\n",
      "Epoch 850/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0291, Test Acc: 0.8501\n",
      "Epoch 900/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0422, Test Acc: 0.8501\n",
      "Epoch 950/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0554, Test Acc: 0.8499\n",
      "Epoch 1000/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0677, Test Acc: 0.8498\n",
      "Epoch 1050/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0793, Test Acc: 0.8496\n",
      "Epoch 1100/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 1.0905, Test Acc: 0.8496\n",
      "Epoch 1150/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1012, Test Acc: 0.8495\n",
      "Epoch 1200/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1124, Test Acc: 0.8494\n",
      "Epoch 1250/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1228, Test Acc: 0.8493\n",
      "Epoch 1300/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1328, Test Acc: 0.8492\n",
      "Epoch 1350/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1431, Test Acc: 0.8490\n",
      "Epoch 1400/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1527, Test Acc: 0.8489\n",
      "Epoch 1450/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1622, Test Acc: 0.8489\n",
      "Epoch 1500/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1712, Test Acc: 0.8488\n",
      "Epoch 1550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1805, Test Acc: 0.8486\n",
      "Epoch 1600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1896, Test Acc: 0.8486\n",
      "Epoch 1650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.1980, Test Acc: 0.8486\n",
      "Epoch 1700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2069, Test Acc: 0.8485\n",
      "Epoch 1750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2152, Test Acc: 0.8485\n",
      "Epoch 1800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2234, Test Acc: 0.8484\n",
      "Epoch 1850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2318, Test Acc: 0.8484\n",
      "Epoch 1900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2399, Test Acc: 0.8483\n",
      "Epoch 1950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2477, Test Acc: 0.8483\n",
      "Epoch 2000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 1.2557, Test Acc: 0.8482\n",
      "\n",
      "Training configuration: ReLU_MSE\n",
      "Epoch 1/2000 -> Train Loss: 0.0895, Train Acc: 0.1950, Test Loss: 0.0881, Test Acc: 0.2231\n",
      "Epoch 50/2000 -> Train Loss: 0.0029, Train Acc: 0.9867, Test Loss: 0.0204, Test Acc: 0.8607\n",
      "Epoch 100/2000 -> Train Loss: 0.0006, Train Acc: 0.9967, Test Loss: 0.0218, Test Acc: 0.8540\n",
      "Epoch 150/2000 -> Train Loss: 0.0001, Train Acc: 1.0000, Test Loss: 0.0221, Test Acc: 0.8544\n",
      "Epoch 200/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0223, Test Acc: 0.8544\n",
      "Epoch 250/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0225, Test Acc: 0.8543\n",
      "Epoch 300/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0226, Test Acc: 0.8541\n",
      "Epoch 350/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0227, Test Acc: 0.8541\n",
      "Epoch 400/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0228, Test Acc: 0.8540\n",
      "Epoch 450/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0229, Test Acc: 0.8542\n",
      "Epoch 500/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0230, Test Acc: 0.8541\n",
      "Epoch 550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0230, Test Acc: 0.8540\n",
      "Epoch 600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0231, Test Acc: 0.8540\n",
      "Epoch 650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0231, Test Acc: 0.8539\n",
      "Epoch 700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0232, Test Acc: 0.8538\n",
      "Epoch 750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0232, Test Acc: 0.8537\n",
      "Epoch 800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0233, Test Acc: 0.8536\n",
      "Epoch 850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0233, Test Acc: 0.8536\n",
      "Epoch 900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0233, Test Acc: 0.8535\n",
      "Epoch 950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0234, Test Acc: 0.8535\n",
      "Epoch 1000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0234, Test Acc: 0.8535\n",
      "Epoch 1050/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0234, Test Acc: 0.8535\n",
      "Epoch 1100/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0235, Test Acc: 0.8536\n",
      "Epoch 1150/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0235, Test Acc: 0.8535\n",
      "Epoch 1200/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0235, Test Acc: 0.8535\n",
      "Epoch 1250/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0236, Test Acc: 0.8534\n",
      "Epoch 1300/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0236, Test Acc: 0.8534\n",
      "Epoch 1350/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0236, Test Acc: 0.8533\n",
      "Epoch 1400/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0236, Test Acc: 0.8534\n",
      "Epoch 1450/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0236, Test Acc: 0.8533\n",
      "Epoch 1500/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0237, Test Acc: 0.8533\n",
      "Epoch 1550/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0237, Test Acc: 0.8532\n",
      "Epoch 1600/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0237, Test Acc: 0.8532\n",
      "Epoch 1650/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0237, Test Acc: 0.8531\n",
      "Epoch 1700/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0238, Test Acc: 0.8530\n",
      "Epoch 1750/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0238, Test Acc: 0.8530\n",
      "Epoch 1800/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0238, Test Acc: 0.8531\n",
      "Epoch 1850/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0238, Test Acc: 0.8530\n",
      "Epoch 1900/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0238, Test Acc: 0.8531\n",
      "Epoch 1950/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0239, Test Acc: 0.8531\n",
      "Epoch 2000/2000 -> Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0239, Test Acc: 0.8533\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Fonction d'accuracy\n",
    "def accuracy(yhat, y):\n",
    "    assert len(y.shape) == 1 or y.size(1) == 1\n",
    "    return (torch.argmax(yhat, dim=1) == y).double().mean()\n",
    "\n",
    "# Fonction pour convertir en one-hot\n",
    "def to_one_hot(y, num_classes):\n",
    "    return F.one_hot(y, num_classes=num_classes).float()\n",
    "\n",
    "def train_and_log(model, train_loader, test_loader, criterion, optimizer, epochs, writer, output_dim):\n",
    "    for epoch in range(epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0, 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Aplatir les données si nécessaire\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "            \n",
    "            yhat = model(x_batch)\n",
    "\n",
    "            # Si on utilise MSELoss, convertir les cibles en one-hot\n",
    "            if isinstance(criterion, nn.MSELoss):\n",
    "                y_batch = to_one_hot(y_batch, num_classes=output_dim)\n",
    "            \n",
    "            loss = criterion(yhat, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += accuracy(yhat, y_batch if isinstance(criterion, nn.CrossEntropyLoss) else torch.argmax(y_batch, dim=1)).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "\n",
    "        # Mode évaluation - toutes les 50 époques\n",
    "        if (epoch + 1) % 50 == 0 or epoch == epochs - 1 or epoch == 0:\n",
    "            model.eval()  # Activer le mode évaluation (Dropout désactivé)\n",
    "            test_loss, test_acc = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in test_loader:\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                    # Aplatir les données si nécessaire\n",
    "                    x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "\n",
    "                    yhat = model(x_batch)\n",
    "\n",
    "                    # Si on utilise MSELoss, convertir les cibles en one-hot\n",
    "                    if isinstance(criterion, nn.MSELoss):\n",
    "                        y_batch = to_one_hot(y_batch, num_classes=output_dim)\n",
    "\n",
    "                    loss = criterion(yhat, y_batch)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    test_acc += accuracy(yhat, y_batch if isinstance(criterion, nn.CrossEntropyLoss) else torch.argmax(y_batch, dim=1)).item()\n",
    "\n",
    "            test_loss /= len(test_loader)\n",
    "            test_acc /= len(test_loader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "            # Enregistrement dans TensorBoard\n",
    "            writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Test\": test_loss}, epoch)\n",
    "            writer.add_scalars(\"Accuracy\", {\"Train\": train_acc, \"Test\": test_acc}, epoch)\n",
    "\n",
    "\n",
    "# Classe LinearMultiClass avec ajout de Dropout\n",
    "class LinearMultiClass(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=None, finalActivation=None, activation=nn.Tanh, dropout_prob=0):\n",
    "        super(LinearMultiClass, self).__init__()\n",
    "        \n",
    "        if layers is None:\n",
    "            layers = []\n",
    "\n",
    "        self.layers = nn.ModuleList()  # Utilisé pour accéder aux modules individuels\n",
    "        input_dim = inSize\n",
    "\n",
    "        # Ajouter les couches cachées\n",
    "        for hidden_dim in layers:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            self.layers.append(activation())\n",
    "            self.layers.append(nn.Dropout(p=dropout_prob))  # Ajouter Dropout après chaque activation\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Ajouter la couche de sortie\n",
    "        self.layers.append(nn.Linear(input_dim, outSize))\n",
    "\n",
    "        # Ajouter la fonction d'activation finale si elle est définie\n",
    "        if finalActivation is not None:\n",
    "            self.layers.append(finalActivation)\n",
    "\n",
    "        # Utilisation de nn.Sequential pour empiler les couches\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Comparaison des modèles\n",
    "def compare_models(train_loader, test_loader, input_dim, output_dim, epochs=2000):\n",
    "    # Configurations à comparer\n",
    "    configs = [\n",
    "        {\"activation\": nn.Tanh, \"criterion\": nn.MSELoss(), \"tag\": \"Tanh_MSE\"},\n",
    "        {\"activation\": nn.Tanh, \"criterion\": nn.CrossEntropyLoss(), \"tag\": \"Tanh_CrossEntropy\"},\n",
    "        {\"activation\": nn.ReLU, \"criterion\": nn.CrossEntropyLoss(), \"tag\": \"ReLU_CrossEntropy\"},\n",
    "        {\"activation\": nn.ReLU, \"criterion\": nn.MSELoss(), \"tag\": \"ReLU_MSE\"},\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        # Utiliser Softmax comme finalActivation seulement si on utilise CrossEntropyLoss\n",
    "        final_activation = None if isinstance(config[\"criterion\"], nn.CrossEntropyLoss) else nn.Softmax(dim=1)\n",
    "\n",
    "        # Initialisation du modèle\n",
    "        model = LinearMultiClass(\n",
    "            inSize=input_dim,\n",
    "            outSize=output_dim,\n",
    "            layers=[100,100],\n",
    "            finalActivation=final_activation,\n",
    "            activation=config[\"activation\"]\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialisation de l'optimiseur et du critère\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        criterion = config[\"criterion\"]\n",
    "        \n",
    "        # TensorBoard Writer\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{config['tag']}\")\n",
    "        \n",
    "        print(f\"\\nTraining configuration: {config['tag']}\")\n",
    "        \n",
    "        # Entraînement\n",
    "        train_and_log(model, train_loader, test_loader, criterion, optimizer, epochs, writer, output_dim)\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "# Paramètres du dataset et de l'entraînement\n",
    "input_dim = train_loader.dataset[0][0].numel()\n",
    "output_dim = 10  # Nombre de classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "compare_models(train_loader, test_loader, input_dim, output_dim, epochs=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des résultats dans TensorBoard\n",
    "### Relu\n",
    "#### CrossEntropy\n",
    "![Accuracy](./img/accuracy_relu_cross.png)\n",
    "![Loss](./img/loss_relu_cross.png)\n",
    "\n",
    "![Accuracy](./img/accuracy_relu_mse.png)\n",
    "![Loss](./img/loss_relu_mse.png)\n",
    "\n",
    "### Tanh\n",
    "#### CrossEntropy\n",
    "![Accuracy](./img/acccuracy_than_cross.png)\n",
    "![Loss](./img/loss_than_cross.png)\n",
    "\n",
    "#### MSELoss\n",
    "![Accuracy](./img/accuracy_than_mse.png)\n",
    "![Loss](./img/loss_than_mse.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"alert-danger\"> On observe du sur-apprentissage avec un ecart d'un 15aine de pourcent entre le train et le test pour l'accuracy et une loss qui remonte en test après un certain nombre d'époque pour les modèles CrossEntropy. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span class=\"alert-success\"> Exercice : Régularisation des réseaux </span>\n",
    "\n",
    "### Pénalisation des couches\n",
    "Une première technique pour éviter le sur-apprentissage est de régulariser chaque couche par une pénalisation sur les poids, i.e. de favoriser des poids faibles. On parle de pénalisation L1 lorsque la pénalité est de la forme $\\|W\\|_1$ et L2 lorsque la norme L2 est utilisée : $\\|W\\|_2^2$. En pratique, cela consiste à rajouter à la fonction de coût globale du réseau un terme en $\\lambda Pen(W)$ pour les paramètres de chaque couche que l'on veut régulariser.\n",
    "\n",
    "Expérimentez avec une norme L2 dans $\\{0,10^{-5},10^{-4},10^{-3},10^{-2},\\}$, l'évolution de la pénalisation et du coût en fonction du nombre d'époques. Vous pouvez aussi observer les histogrammes de la distribution des poids des différentes couches en utilisant la fonction addWeightsHisto ci dessous.  Utilisez pour ces experiences un réseau à 3 couches chacune de taille 100 et un coût de CrossEntropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requiert que les modules soient enregistrés dans une liste model.layers\n",
    "def addWeightsHisto(writer,model,epoch):                \n",
    "    ix = 0\n",
    "    for module in model.layers:\n",
    "        if isinstance(module, nn.Linear):\n",
    "           writer.add_histogram(f'linear/{ix}/weight',module.weight, epoch)\n",
    "           ix += 1\n",
    "\n",
    "# Comparaison des modèles avec régularisation L2\n",
    "def compare_models_with_regularization(train_loader, test_loader, input_dim, output_dim, epochs=2000):\n",
    "    # Liste des valeurs de régularisation L2 à tester\n",
    "    lambdas = [0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "    config = {\"activation\": nn.Tanh, \"criterion\": nn.CrossEntropyLoss(), \"tag\": \"Tanh_CrossEntropy\"}\n",
    "    \n",
    "    for l2_penalty in lambdas:\n",
    "        # Initialisation du modèle\n",
    "        model = LinearMultiClass(\n",
    "            inSize=input_dim,\n",
    "            outSize=output_dim,\n",
    "            layers=[100,100],  # Quatre couches cachées pour la complexité\n",
    "            finalActivation=nn.Softmax(dim=1),  # Softmax pour la classification multi-classe\n",
    "            activation=config[\"activation\"]\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialisation de l'optimiseur avec pénalisation L2 (weight_decay)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=l2_penalty)\n",
    "        criterion = config[\"criterion\"]\n",
    "        \n",
    "        # TensorBoard Writer pour visualisation\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{config['tag']}_weight_decay_{l2_penalty}\")\n",
    "        \n",
    "        print(f\"\\nTraining configuration: {config['tag']}, L2 Penalty: {l2_penalty}\")\n",
    "        \n",
    "        # Entraînement du modèle et journalisation\n",
    "        train_and_log(model, train_loader, test_loader, criterion, optimizer, epochs, writer, output_dim)\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "# Paramètres du dataset et de l'entraînement\n",
    "input_dim = train_loader.dataset[0][0].numel()\n",
    "output_dim = 10  # Nombre de classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Comparaison des modèles avec différentes régularisations L2\n",
    "compare_models_with_regularization(train_loader, test_loader, input_dim, output_dim, epochs=2000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des résultats dans TensorBoard\n",
    "### Penalité 0\n",
    "![Accuracy](./img/penality0.png)\n",
    "\n",
    "### Penalité 1e-5\n",
    "![Accuracy](./img/penality1e-5.png)\n",
    "\n",
    "### Penalité 1e-4\n",
    "![Accuracy](./img/penality1e-4.png)\n",
    "\n",
    "### Penalité 1e-3\n",
    "![Accuracy](./img/penality1e-3.png)\n",
    "\n",
    "### Penalité 1e-2\n",
    "![Accuracy](./img/penality1e-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Une autre technique très utilisée est le <a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html> **Dropout** </a>. L’idée du Dropout est proche du moyennage de modèle : en entraînant k modèles de manière indépendante, on réduit la variance du modèle. Entraîner k modèles présente un surcoût non négligeable, et l’intérêt du Dropout est de réduire la complexité mémoire/temps de calcul. Le Dropout consiste à chaque itération à *geler* certains neurones aléatoirement dans le réseau en fixant leur sortie à zéro. Cela a pour conséquence de rendre plus robuste le réseau.\n",
    "\n",
    "Le comportement du réseau est donc différent en apprentissage et en inférence. Il est obligatoire d'utiliser ```model.train()``` et ```model.eval()``` pour différencier les comportements.\n",
    "Testez sur quelques réseaux pour voir l'effet du dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles avec différentes probabilités de Dropout\n",
    "def compare_models_with_dropout(train_loader, test_loader, input_dim, output_dim, epochs=30):\n",
    "    # Liste des probabilités de Dropout à tester\n",
    "    dropout_probs = [0, 0.1, 0.3, 0.5]\n",
    "    config = {\"activation\": nn.Tanh, \"criterion\": nn.CrossEntropyLoss(), \"tag\": \"Tanh_CrossEntropy\"}\n",
    "    \n",
    "    for dropout_prob in dropout_probs:\n",
    "        # Initialisation du modèle\n",
    "        model = LinearMultiClass(\n",
    "            inSize=input_dim,\n",
    "            outSize=output_dim,\n",
    "            layers=[100,100],  # Quatre couches cachées pour la complexité\n",
    "            finalActivation=nn.Softmax(dim=1),  # Softmax pour la classification multi-classe\n",
    "            activation=config[\"activation\"],\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialisation de l'optimiseur\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = config[\"criterion\"]\n",
    "        \n",
    "        # TensorBoard Writer pour visualisation\n",
    "        writer = SummaryWriter(log_dir=f\"runs/{config['tag']}_dropout_{dropout_prob}\")\n",
    "        \n",
    "        print(f\"\\nTraining configuration: {config['tag']}, Dropout Probability: {dropout_prob}\")\n",
    "        \n",
    "        # Entraînement du modèle et journalisation\n",
    "        train_and_log(model, train_loader, test_loader, criterion, optimizer, epochs, writer, output_dim)\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "# Paramètres du dataset et de l'entraînement\n",
    "input_dim = train_loader.dataset[0][0].numel()\n",
    "output_dim = 10  # Nombre de classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Comparaison des modèles avec différentes probabilités de Dropout\n",
    "compare_models_with_dropout(train_loader, test_loader, input_dim, output_dim, epochs=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des résultats dans TensorBoard\n",
    "### Dropout 0\n",
    "![Accuracy](./img/dropout0.png)\n",
    "\n",
    "### Dropout 0.1\n",
    "![Accuracy](./img/dropout0.1.png)\n",
    "\n",
    "### Dropout 0.3\n",
    "![Accuracy](./img/dropout0.3.png)\n",
    "\n",
    "### Dropout 0.5\n",
    "![Accuracy](./img/dropout0.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "\n",
    "On sait que les données centrées réduites permettent un apprentissage plus rapide et stable d’un modèle ; bien qu’on puisse faire en sorte que les données en entrées soient centrées réduites, cela est plus délicat pour les couches internes d’un réseau de neurones. La technique de <a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> **BatchNorm**</a> consiste à ajouter une couche qui a pour but de centrer/réduire les données en utilisant une moyenne/variance glissante (en inférence) et les statistiques du batch (en\n",
    "apprentissage).\n",
    "\n",
    "Tout comme pour le dropout, il est nécessaire d'utiliser ```model.train()``` et ```model.eval()```. \n",
    "Expérimentez la batchnorm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles avec BatchNorm\n",
    "def compare_models_with_batchnorm(train_loader, test_loader, input_dim, output_dim, epochs=2000):\n",
    "    # Liste des probabilités de Dropout à tester\n",
    "    dropout_probs = [0, 0.1, 0.3, 0.5]\n",
    "    config = {\"activation\": nn.Tanh, \"criterion\": nn.CrossEntropyLoss(), \"tag\": \"Tanh_CrossEntropy_BatchNorm\"}\n",
    "    model = LinearMultiClass(\n",
    "        inSize=input_dim,\n",
    "        outSize=output_dim,\n",
    "        layers=[100,100],  \n",
    "        finalActivation=nn.Softmax(dim=1), \n",
    "        activation=config[\"activation\"],\n",
    "        BatchNorm=True\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialisation de l'optimiseur\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # TensorBoard Writer pour visualisation\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{config['tag']}\")\n",
    "    \n",
    "    print(f\"\\nTraining configuration: {config['tag']}\")\n",
    "    \n",
    "    # Entraînement du modèle et journalisation\n",
    "    train_and_log(model, train_loader, test_loader, config[\"criterion\"], optimizer, epochs, writer, output_dim)\n",
    "    \n",
    "    writer.close()\n",
    "        \n",
    "        \n",
    "input_dim = train_loader.dataset[0][0].numel()\n",
    "output_dim = 10  # Nombre de classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Comparaison des modèles avec différentes probabilités de Dropout\n",
    "compare_models_with_batchnorm(train_loader, test_loader, input_dim, output_dim, epochs=2000)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des résultats dans TensorBoard\n",
    "### BatchNorm\n",
    "![Accuracy](./img/batchnorm.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
